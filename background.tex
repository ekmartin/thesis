\chapter{Background}\label{chap:background}

\section{Soup}

Building sophisticated web applications while scaling to potentially millions of
users forces developers to compromise between performance, user requirements,
and application complexity. Whereas traditional relational databases logically
are able to fulfill the increasingly complex storage demands of today's internet
businesses, they are far from able to do so at the scale and performance
required. To continue serving requests at increasing throughputs with low
latencies, developers introduce mitigation strategies ranging from complex cache
hierarchies~\cite{memcached} to denormalized schemas~\cite{denormalization}.

These methods are usually used to drastically improve read performance, while
penalizing write throughput and increasing application complexity. Soup sets out
to solve this dilemma once and for all, with a structured storage system capable
of horizontally scaling to millions of reads per second, without the need for
complex cache deployments or manual maintenance of materialized views.

\subsection{Data-flow}

Soup achieves this through use of an incrementally maintained data-flow graph.
New updates propagate through the graph at write-time, with pre-computed results
stored at selected \textit{materialized} nodes throughout the graph. This
moves the bulk of the workload from reads to writes, by allowing read operations
to directly access computed state from materialized nodes further down the
graph.

\todo{figure that compares a traditional query graph to soup's data-flow}

Applications using Soup define base table schemas and a set of queries ahead of
time. Whereas the former is common in traditional relational database management
systems, the latter is not, and is the primary source of Soup's read performance
improvements. A relational database computes the result of queries on-the-fly by
parsing queries into a query-graph, which it then executes. This requires
potentially costly computations to be performed multiple times for separate
queries, while throwing away intermediary results that could be re-used. Soup
instead builds a \textit{data-flow} graph from its pre-defined queries,
propagating each update through it at write-time. Computations can then be
incrementally maintained on each write, reducing the work needed by a
read-operation to something more similar to a simple key-value read in a caching
system.

\begin{listing}[H]
  \begin{minted}[frame=lines]{sql}
CREATE TABLE Car (id int, brand varchar(255), PRIMARY KEY(id));
QUERY CountCars: SELECT COUNT(*) FROM Car WHERE brand = ?;
  \end{minted}

  \caption{An example base table with a corresponding
  query.}\label{lst:example-schema}
\end{listing}

Implemented naively, incrementally maintaining a data-flow graph for each query
would have disastrous storage consequences. Each query would need a separate
graph, duplicating data across a range of nodes. Instead, Soup builds a single
data-flow graph from all of its queries, recognizing common sub-expressions
where possible~\cite{common-expression}. That still leaves the issue of what
state to incrementally maintain. With queries consisting of a potentially large
amount of nodes, materializing data at each step would lead to significant
overlap between nodes. Instead, Soup only materializes and incrementally
maintains state at nodes it considers \textit{stateful}, with other nodes
referring upwards in the graph to its closest materialized ancestor node.

The state at these materialized nodes is also kept \textit{partial} whenever
possible. Instead of storing the results for all queries---like a materialized
view does---Soup only retains state for records in the application's working
set, evicting rarely used data. Queries for missing keys result in ancestor
queries---\textit{replays}---to nodes further up the graph. Similar to cache
misses in other systems these eventually propagate all the way up to the base
nodes, where a full copy of the state is always maintained.

\subsubsection{Migrations}

Schema migrations are inevitable in long-running applications: business
requirements change, projects are refactored, and new features are added.
Performing migrations on traditional database systems, without downtime, is on
the other hand far from trivial~\cite{stripe, gh-ost}. While Soup requires both
the schema \textit{and} all queries to be defined, it handles changes in both
seamlessly.

Added queries extend the existing data-flow with additional nodes, while
re-using as much as possible of the existing graph. New partially materialized
nodes can start serving requests right away, by fetching data from ancestor
nodes if necessary. This is not the case with fully materialized nodes, which
require a full representation of their state at all times. To bring these nodes
online, Soup incurs a full replay of all the state needed, potentially delaying
new requests until all replays have completed. Review of all the migrations
performed during the lifetime of the HotCRP conference review
program~\cite{hotcrp}---which uses MySQL---showed that these delays were rare,
with Soup being able to transition 95\% of all schema changes without downtime.

\todo{figure of a changing data-flow graph}

Changes to the base table schema happen in-place, and Soup's base nodes retain a
full history of columns for each table. This ensures that both existing and new
requests can be served alongside each-other, allowing the data-flow graph to
transition without downtime.

\subsubsection{Domains}

Soup's data-flow graph is partitioned into \textit{domains}, each containing a
series of nodes. Updates are processed at separate domains asynchronously, in
different computational units---threads, or other machines altogether for
distributed Soup. Within a single domain, packets are processed synchronously,
one at a time, removing the need for locking within domains. Packet processing
at domains include both regular updates and other types of packets, such as
replay requests.

\todo{figure that shows different domains}

Domains are separated by egress and ingress nodes, responsible for maintaining
communication across domain boundaries using a buffered channel. When Soup is
run in a distributed fashion, communication between separate domains happen
here, over TCP sockets.\@

\subsubsection{Sharding}

Distributing domains across computational units lets Soup divide its
computational load between a cluster of machines. This is only an improvement if
the load is uniformly spread across all domains. When that is not the case, and
a majority of the data is skewed towards a single domain, Soup is left to
processing most of the requests in a single computational unit. This is where
sharding comes in. By splitting the atom that is a single domain into multiple
shards, both the computational load and the data stored at that domain can be
spread between multiple computational units.

\todo{show a figure with a sharder?}

Balancing the data across a cluster is essential in scaling Soup to larger
data-sets. Without it, Soup's capabilities would be capped at the memory size of
the largest machine in the cluster. Soup shards data by hash-partitioning keys
statically. This is unfortunate for workloads skewed towards a small key subset,
where only a few domains might end up serving most of the requests received.
Dynamic sharding would help here and is a future goal for Soup.

When necessary, a \textit{sharder} node is inserted between domains, translating
between sharding schemes in separate domains. This allows nodes further down in
the graph to remain partial, at the cost of having to replay state across
domains.


\subsubsection{Evictions}

To ensure that partial state does in fact remain partial over time, Soup evicts
data when necessary. Currently this happens when Soup's memory-limit goes beyond
an application-defined limit. This triggers an eviction notice sent to the
largest domain---measured in state-size---which then takes care of propagating
this eviction notice downstream in the graph to any nodes that might depend on
the evicted state.

\todo{might not be necessary, but could have an eviction figure here}

The keys to evict at a specific node are picked randomly. In the future, this
could be improved through more sophisticated eviction strategies, such as only
evicting the least-recently used records.

\subsection{Operators}

Soup's data-flow graph consists of relational operators, where each operator
emits either the \code{Positive} or the \code{Negative} records required for
downstream nodes to maintain their state accordingly.

\subsubsection{Base nodes}

Every packet that reaches Soup is first injected into an appropriate base node,
similar to a table in a relational database. This is where external API requests
are translated into a language the rest of the data-flow graph understands.
While clients might issue \eg deletion requests by \textit{key}, the base nodes
translate the request into a \code{Negative} record for the entire row, which
the rest of the data-flow graph can use to invalidate removed state. Update
requests are likewise first translated into a \code{Negative} record, followed
by a \code{Positive} record containing the new row.

Whereas other stateful nodes can keep their state \textit{partial}, choosing
which records to maintain and which to discard, base nodes always remain fully
materialized at all times.

\subsubsection{Stateless operators}

Stateless operators process updates with no regard for prior events, without
maintaining any state at all. The operators that fall within this category are
\textit{pure functions}, such as the \textit{projection} and \textit{filter}
operators. The former pick out one or more fields from each incoming row, while
the latter determines if a row should be forwarded through the data-flow graph
or not.

While Soup is free to insert operators into its data-flow graph as it sees fit,
both projections and filters can often be mapped directly to a part of an SQL
query. \code{SELECT name \dots} would result in the projection shown in
figure~\ref{fig:project}, while a \code{WHERE}-clause on the form of \code{WHERE
age < 10} would produce a filter operator emitting only records where \code{age
< 10} is true.

\begin{figure}[H]
  \centering
  \includesvg[width=0.3\textwidth]{project}
  \caption{A projection operator responsible for picking out the \code{name}
  column of incoming records.}\label{fig:project}
\end{figure}

\subsubsection{Stateful operators}

Whereas stateless operators resemble their counter-parts in relational
databases~\cite{codd}, Soup's stateful operators compute results incrementally
by maintaining internal state between updates. This saves Soup from
re-doing potentially expensive computations, by instead mutating the previous
result when a new record arrives.

The \textit{count} operator---produced by a SQL \code{COUNT}-clause---is an
example of a stateful operator, where \code{Positive} and \code{Negative}
records incur an addition or subtraction of the current count, respectively.
Another example is the \textit{top-k} operator, perhaps produced by an
\code{ORDER BY}-clause coupled with a \code{LIMIT} to determine the $ k $
most significant or insignificant elements.

\begin{figure}[H]
  \centering
  \includesvg[width=0.3\textwidth]{max}
  \caption{\
    The \textit{max} operator emits both a negative and a positive record when
    its state changes: the former to signal that its previous state should be
    invalidated, the latter to inform downstream nodes of the new result.
  }
\end{figure}

\subsubsection{Joins}

Finally, Soup supports joining together multiple paths of the data-flow graph.
This is made possible using \textit{ancestor queries}. Whenever a record arrives
at a join operator, it queries its other ancestors for the records required to
produce a single, unified update. To make sure that this is completed in an
efficient manner, join operators force their ancestors to retain indices on the
fields that they need to be queried for.

\begin{figure}[H]
  \centering
  \includesvg[width=0.7\textwidth]{join}
  \caption{\
    An ancestor query is performed to the right side of the join to produce a
    unified record for the update received from the left.
  }
\end{figure}

\subsection{Eventual consistency}

Databases are often considered the source of truth for applications, and
anomalies here could have disastrous consequences. Whereas fatal failures are
easy to recognize, unexpected behavior at the data layer could be the exact
opposite. This is a convincing argument for strong consistency, where the result
of all operations can be reasoned about from the ordering and type of operation
performed.

Regardless, companies scaling web applications to large amounts of users often
opt for systems with lesser consistency guarantees~\cite{dynamo, pnuts, werner}.
While the performance of strongly consistent systems have taken a turn for the
better with the introduction of horizontally systems with clearly defined
consistency guarantees~\cite{spanner, cockroach}, the race is still far from
even when compared to systems with lesser consistency guarantees. At the same
time, the question of whether strong consistency is actually a requirement for
most applications remain. Analysis of live requests at
Facebook~\cite{existential} showed the opposite: only 0.0004\% of reads would
have returned different results in a strongly consistent system with total
ordering. These cases could then be handled explicitly, avoiding the need to
penalize the performance of the entire system for a fraction of the requests.

Soup targets applications where eventual consistency is sufficient, and would
not be able to provide the performance it does without it. Eventual consistency
avoids the need for explicit synchronization on every update, while allowing
Soup to scale in a distributed fashion without a total ordering of writes.
Clients receive write acknowledgments for writes when the update has been safely
persisted to durable storage, after which it propagates through Soup's data-flow
graph asynchronously.

Reads access double-buffered hash tables directly~\cite{evmap}, without the need
for locks. Writes update one of the buffers, and expose updated content to the
readers with an atomic swap.

\subsection{Architecture}

\todo{figure}

\subsubsection{Controller}

At the heart of Soup lies a replicated controller, with a leader elected
using ZooKeeper~\cite{zookeeper}. The controller is the first point of contact
for Soup's external APIs, and is responsible for managing meta state, such as
schemas and queries.

\subsubsection{Souplets}

Processing of updates happen within the Souplets---Soup's worker nodes. Each
Souplet includes a pool of threads which together go through the incoming
packets for its domains. While maximum one thread can process updates for a
domain at once, multiple domains can process updates in parallel. Earlier
versions of Soup ran domains in separate threads altogether, resulting in a
core-constrained system when the number of domains went far beyond the host's
CPU core count.

Communication between the Souplets and the controller happens over TCP, at the
coordination layer.

\subsubsection{Readers}

Soup's reader nodes make use of double-buffered hash tables to make it possible
to read and write to a single data structure at the same time. To prevent high
throughput write processing from slowing down reads, read requests are processed
by separate threads---readers.

\subsection{Interacting with Soup}

Applications using Soup define a base table schema and a set of corresponding
queries. The query syntax resembles that of prepared statements in relational
databases, where placeholders are replaced with values when the query is used in
a read operation. Both the schema and the queries can be modified and extended
later on, through Soup's external API.\@

\begin{listing}[H]
  \begin{minted}[frame=lines]{sql}
/* Base table schemas: */
CREATE TABLE Article (aid int, title varchar(255),
                     url text, PRIMARY KEY(aid));
CREATE TABLE Vote (aid int, uid int);

/* Intermediate view (not exposed through the client): */
VoteCount: SELECT Vote.aid, COUNT(uid) AS votes
            FROM Vote GROUP BY Vote.aid;

/* Read query: */
QUERY ArticleWithVoteCount:
            SELECT Article.aid, title, url, VoteCount.votes AS votes
            FROM Article, VoteCount
            WHERE Article.aid = VoteCount.aid AND Article.aid = ?;
  \end{minted}
  \caption{Soup schema with two base tables and an external query.}\label{lst:soup-schema}
\end{listing}

Writing to and reading from Soup is done through mutators and getters. Both are
built by going through the controller, after which writes can go directly to the
domain and reads can access readers directly.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
// Build mutators and getter.
let mut article = blender.get_mutator("Article").unwrap();
let mut vote = blender.get_mutator("Vote").unwrap();
let mut awvc = blender.get_getter("ArticleWithVoteCount").unwrap();

// Insert a new article:
let aid = 1;
let title = "new article";
let url = "https://ntnu.edu";
article
  .put(vec![aid.into(), title.into(), url.into()])
  .unwrap();

// Vote for the article:
let uid = 123;
vote
  .put(vec![aid.into(), uid.into()])
  .unwrap();

// Read the vote count:
println!("{}", awvc.lookup(&[aid.into()], true));
  \end{minted}

  \caption{Soup example usage, where an article and a vote is inserted,
  followed by a read of the vote count.}\label{lst:soup-api}
\end{listing}

\subsection{MySQL Shim}\label{sec:mysql-shim}

Soup supports a decent subset of SQL in its query definitions. Regardless, using
Soup in an application requires significant changes: all queries have to be
defined ahead of time, and interactions with Soup have to go through Soup's
external API.\@ Soup's MySQL shim~\cite{soup-mysql} makes this easier by letting
applications communicate with Soup using the MySQL binary protocol.

The MySQL shim acts as a separate service, which clients interact with over
TCP/IP.\@ Received queries extend the Soup data-flow graph if needed, before
executing the query by forwarding it to an appropriate Soup worker.

\section{Rust}
\subsection{Foreign Function Interface}

\section{Recovery}

\section{bincode}

bincode~\cite{bincode} is a Rust binary serialization library, used heavily
throughout both this thesis and in Soup, for everything from RPC
communication to persisting data to durable storage. In short, bincode takes an
arbitrary Rust object and turns it into a series of bytes---an encoded object.
The size of the resulting byte stream is usually either less than, or the same
as, the size of the source object. bincode builds on top of the
Serde\furl{https://serde.rs/} serialization framework.

As the encoded format is of relevance to later sections, we will briefly go
through it here. Primitive values, such as numbers, are encoded directly using
Rust's \code{Writer} trait, with a few exceptions:

\begin{itemize}
  \item \code{isize} and \code{usize} types, which have varying sizes depending
    on the OS, are encoded as \code{i32} and \code{u64} correspondingly.
  \item Strings are encoded as the tuple \code{(number of bytes, bytes)}, where
    the former is a \code{u64} and the latter is a byte slice.
\end{itemize}

Compound types---enums, structs, vectors, and tuples---are encoded recursively,
with each of their fields placed out in succession. With vector lengths not
being determined at compile time, vectors are prefixed with a length field on
the form of a \code{u64}. This is not necessary for the other compound types, as
their sizes do not vary at runtime. An enum instance can represent multiple
types, and is prefixed with a \code{u32} tag used to determine which
enum variant it represents.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
#[derive(Serialize, Deserialize, Debug, PartialEq)]
enum Number {
    Positive(u64),
    Negative(u64)
}

fn main() {
    let values = vec![Number::Positive(3), Number::Negative(4)];
    // This serializes as:
    // vector length u64,
    // + enum variant u32 + u64,
    // + enum variant u32 + u64
    // = u64, u32, u64, u32, u64
    // = 32 bytes
    let raw = bincode::serialize(&values).unwrap();
    let deserialized: Vec<_> = bincode::deserialize(&raw).unwrap();

    for (i, element) in values.into_iter().enumerate() {
        assert_eq!(element, deserialized[i]);
    }
}
  \end{minted}

  \caption{}\label{lst:bincode}
\end{listing}

\section{Rust}

\subsection{Foreign Function Interface}

Rust has excellent support for calling into external C programs.

\section{Profiling}

\subsection{CPU}

A large part of application performance tuning comes down to figuring out which
portion of a program is running slowly and why that is the case. Throughout
this thesis that is accomplished using
\code{perf}\furl{https://perf.wiki.kernel.org}---a profiling tool that helps us
answer the question ``What is the CPU spending time on?''.

\code{perf} collects information from both hardware counters and logical
tracepoints. The latter is especially useful for recording call graphs of a
program, which in turn lets us produce flame graphs like the one in
figure~\ref{fig:flame-example} using tools such as
\code{FlameGraph}\furl{https://github.com/brendangregg/FlameGraph} and
\code{FlameScope}\furl{https://github.com/Netflix/flamescope}. Flame graphs show
time spent on the horizontal axis, while showing the call graph vertically.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{flame-example}
  \caption{\
    An example flame graph from events recorded using \code{perf}.
  }\label{fig:flame-example}
\end{figure}

\subsection{Memory}

Memory leaks happen when we continually allocate memory without freeing it. This
can easily happen in languages without dynamic memory allocation, when a
programmer forgets to deallocate some portion of memory after using it. At the
same time it can also happen in garbage collected languages, \eg when
continuously attaching listener functions to an event system without regard for
previous subscriptions. Rust's ownership system largely prevents issues of the
first kind from happening---variables that go out of scope are deallocated
automatically. Regardless, Rust supports calling into arbitrary C-programs (see
section~\ref{sec:ffi}), where anything could happen.

To profile memory leaks, the Valgrind
Massif\furl{http://valgrind.org/docs/manual/ms-manual.html} heap profiler is
used. Massif continuously takes snapshots of the heap, recording what memory is
used for, and where that memory was allocated from. While this is subject to
change in the future, current versions of Rust use the
\code{jemalloc}\furl{http://jemalloc.net/} memory allocator instead of the
system's default allocator. Unfortunately, memory profiling using Valgrind does
not work well with \code{jemalloc}. To resolve this we can instead force Rust to
use the system allocator, at least while profiling.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
#![feature(alloc_system)]
#![feature(global_allocator, allocator_api)]

#[global_allocator]
static ALLOC: std::alloc::System = std::alloc::System;
  \end{minted}

  \caption{Forcing Rust to use the system memory allocator makes it possible to
  profile it using tools such as Valgrind Massif.}\label{lst:system-alloc}
\end{listing}

\section{SQLite}\label{sec:sqlite}
SQLite~\cite{sqlite} is by far the most widely deployed database ever written.
Used in everything from smart phones to cars, with an estimated user count in
the magnitude of multiple billion users, SQLite is everywhere\footnote{Who
uses SQLite? \url{https://www.sqlite.org/mostdeployed.html}}. SQlite is an
embedded database, and requires no extra processes, or even threads, to run.

In a world of unreliable software, SQLite is stable as a rock. It has 100\%
branch test coverage, with a test suite containing millions of different test
cases. SQLite is, and always has been, available in the public domain. As the
name implies, SQLite provides an SQL interface to developers, with decent support
for everything from indices to views. The library itself is written in about 130
thousand lines of C code.

While the main usage of SQLite is as a persistent application store (\eg in
browsers and mobile applications), SQLite is also popularly used as an engine in
other databases. One such example is the recently open-sourced
FoundationDB~\cite{foundation}, which provides a distributed database with full
ACID transactions, where each shard makes use of SQLite at its core.

\subsection{B-trees}\label{sec:btree}
Similar to a significant amount of other relational databases, SQLite makes use
of B-trees~\cite{btree} for its on-disk index structures. This is with good
reason: B-trees are well suited for mediums that perform better with larger
blocks of data, such as traditional spinning hard drives. While it has never
been officially decided what the B in B-tree stands for, a B-tree is a
self-balancing binary tree data structure.

Unlike other tree structures, such as binary search trees, each node in a B-tree
holds multiple values. By keeping the amount of values in a node---the node
size---close to the size of a block on disk, most of a B-tree's operations can
be performed in $ O(\log_b n) $ disk reads, where $ b $ is the maximum number of
entries per block, and $ \log_b n $ the height of the tree. With traditional
storage mediums, where a single disk seek might take multiple milliseconds, this
is extremely important.

\todo{Include a figure of a B-tree and a B+tree perhaps?}

When the term B-tree is used in database systems today, it is usually used to
refer to an improved version of the traditional data structure, and known as a
B+-tree. Whereas the former stores values in all levels of the tree, the
specialized version only does so at the leaf level, with the internal nodes only
containing copies of the keys. Actual records can then be stored in a different
on-disk data structure, with pointers from the leaf nodes, and by introducing
sibling pointers at the leaf node level, range queries can be efficiently
executed by walking the bottom of the tree horizontally.

\subsection{Rollback Journal}\label{sec:sqlite-locks}
SQLite implements support for atomic transactions through the use of a rollback
journal. A historic copy of values prior to changes are kept in a separate
file---the rollback journal---so that they can be copied back to the actual
database file in the event of a \code{ROLLBACK}. Similarly, this file can be
deleted after a \code{COMMIT} of the transaction.

With a rollback journal, SQLite requires a full exclusive lock to be held for
the duration of all mutations to prevent file corruption, blocking any potential
readers from accessing the database. This is the main reason SQLite is commonly
not used as the storage system for applications that require high-performance
concurrent access to their database (\eg web applications backends with multiple
active users): only a single write operation could be performed at the time.
This is not the case with reads, which hold shared locks.

\subsection{Write-ahead Log}\label{sec:sqlite-wal}
In version 3.7.0, SQLite introduced an alternative to the traditional rollback
journal: the write-ahead log~\cite{sqlite-wal}. Maintaining the same atomicity
and durability guarantees, the use of a WAL significantly improves write
performance by catering to more sequential disk access. Additionally, reading
can now co-exist with writing, as writers do no longer block read access.

While the original rollback journal format writes directly to the database file,
maintaining old values in the rollback journal, SQLite in WAL-mode does the
opposite. Updates are appended to the WAL, and copied over to the main database
file when a \textit{checkpoint} is taken. This is also the reason readers can
continue to access the database while writes are happening, as the database file
itself is not mutated, only the WAL.\@

This introduces a slight performance penalty for reads however, as there are now
potentially two sources of truth for all content: the main database file, and
the WAL until a checkpoint happens. The longer the WAL is, the more time has to
be spent searching through it by reads.

\subsection{Interacting with SQLite}
Most applications interact with SQLite through its C-API, compiling SQL
queries into prepared binary statements, which can then be executed efficiently
with different arguments, as shown in listing \ref{lst:sqlite}.

\begin{listing}[H]
  \begin{minted}[frame=lines]{c}
sqlite3 *db;
sqlite3_stmt *statement;
char *err_msg = 0;
sqlite3_open("test.db", &db);
sqlite3_exec(
  db,
  "CREATE TABLE data (id INTEGER PRIMARY KEY)",
  NULL,
  NULL,
  &err_msg
);

// Compile a prepared statement:
sqlite3_prepare_v2(
  db,
  "INSERT INTO data VALUES (?1)",
  -1,
  &statement,
  0
);

// Then insert a single row with the value 10:
int id = 10;
sqlite3_bind_int(statement, 1, id);
  \end{minted}

  \caption{Simple SQLite C-example showing how to write a single row (error
  handling ignored for brevity)}\label{lst:sqlite}
\end{listing}

In addition, most programming languages have at least one popular library for
accessing SQLite, abstracting away the need to directly call into the C-bindings
through more idiomatic APIs for each language.

SQLite also provides a command-line interface, which can be used to directly
read from and modify a database with SQL statements.

\subsection{SQLite from Rust}
Accessing SQLite from Rust can be done through the excellent \code{Rusqlite}
library~\cite{rusqlite}, which
provides a Rust API on top of SQLite's C-bindings.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let conn = Connection::open("test.db").unwrap();
conn.execute(
  "CREATE TABLE data (id INTEGER PRIMARY KEY)",
  &[],
).unwrap();

// Compile a prepared statement:
let statement = conn.prepare("INSERT INTO data VALUES (?1)").unwrap();

// Then insert a single row with the value 10:
let id = 10;
statement.execute(&[&id]).unwrap();

  \end{minted}

  \caption{SQLite example using \code{rusqlite} showing how to write a single
  row.}\label{lst:sqlite-rust}
\end{listing}

\section{RocksDB}\label{sec:rocksdb}
RocksDB is an embedded key-value store optimized for modern flash storage.
RocksDB started out at Facebook, with the goal of making a version of Google's
LevelDB that performed well on modern hardware. Today, RocksDB is used at the
heart of a wide variety of databases, such as CockroachDB~\cite{cockroach},
MyRocks~\cite{myrocks} (Facebook's fork of MySQL) and TiDB~\cite{tidb}.

Traditional B-tree based database systems are often faced with poor write
performance as a result of random writes, which perform worse than sequential
writes on both magnetic and flash based storage mediums. RocksDB, on the other
hand, achieves impressive write performance through the use of immutable
log-structured merge trees~\cite{lsm} (LSM-trees), avoiding the need for random
writes to persistent storage altogether.

Writes are initially only written to a persistent write-ahead log (WAL) and
in-memory data structures referred to as memtables. Later these memtables are
flushed to their equivalent data structures on disk, Static Sorted Tables (SST).
The latter is done by background threads, allowing regular processing to
continue without getting backed by slow writes to persistent storage. Both of
these components originate in Patrick O'Neil's original paper on LSM-trees,
where the in-memory data structure is referred to as $ C_0 $, and the on-disk
structures $ C_{1..n} $.

% TODO:
% * Calling into RocksDB (C-API, rust-rocksdb)
% * Iterators
% * Prefix Iteration/Slice Transform

\subsection{MemTables}
All writes are initially synchronously written to an in-memory data structure---a
memtable---which is later flushed to disk at the point of filling up. Both the
size and the number of memtables can be configured at runtime.

RocksDB's default memtable implementation is a skiplist, with an $ O(\log n) $
bound on inserts, searches and deletes. This can be changed to a series of hash
based implementations, which offer better performance if all operations are done
within a pre-specified key prefix.

\subsection{Static Sorted Tables}
After a memtable reaches a certain size, RocksDB's background threads takes over
and flushes it to persistent storage. This will generate one or more SS-tables
on disk, where each file is sorted. SS-tables are immutable: a new SS-table is
always created, and existing ones are never updated. This ensures that writes
remain sequential.

\subsection{Write-ahead Log}\label{sec:rocksdb-wal}
RocksDB achieves durability through the use of a write-ahead log (WAL). Without
it, data in memtables would be lost at the event of a crash. By default, every
\code{Put} operation results in a write to the RocksDB WAL, with the optional
possibility of waiting for the write to be fully synchronized to the WAL before
returning.

Each memtable corresponds to a WAL-file, which is marked as obsolete obsolete
when the memtable has been safely persisted to disk. Each WAL-file includes a
sequence number, and the files are iterated through in order during recovery.
The WAL itself is built up of a sequence of records, where each record includes
a cyclic redundancy check (CRC) computed hash over the payload to maintain
integrity~\cite{rocksdb-wal}.

Optionally, the WAL can be written to a different directory than the regular
database files. This is essential for production systems that want to maintain a
high write throughput: compactions and memtable flushes can then utilize the
full disk capacity without slowing down the throughput of WAL writes. Even more
drastically, the database files could be written to faster, volatile storage,
relying solely on never-archived WAL-files for (albeit much slower) recovery.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let batch = WriteBatch::default()
batch.put("a", "1");
batch.put("b", "2");

let opts = WriteOptions::default();
opts.set_sync(true);
db.write(batch, &opts);
  \end{minted}

  \caption{Rust code for safely persisting a batch of writes to RocksDB and its
  write-ahead log.}\label{lst:write-batch}
\end{listing}

\code{Put} operations can also be batched into a \code{WriteBatch} (as shown in
listing~\ref{lst:write-batch}), to amortize the cost of synchronizing the WAL
over a larger amount of write operations. This is an atomic operation: either
all the writes in the write batch succeeds, or none do.

\subsection{Basic Operations}
Akin to other key-value databases, RocksDB offers a familiar API of
\code{Put(key, value)}, \code{Get(key)} and \code{Delete(key)}, operating
directly on bytestream values. Both insertions and deletions are purely
sequential: subsequent \code{Put} operations of the same key never backtrack and
overwrite existing keys, and deletions insert tombstone markers to avoid having
to randomly read and mutate previously written values.

Whereas both memtables and SS-tables are sorted, each tree structure might
overlap with another. This is a result of the immutability property, and newly
created SS-tables might contain key ranges already included in existing
structures. This means that read operations in RocksDB, and other LSM-tree based
storage systems, have to iterate through each tree structure---starting with the
memtables---in an attempt to find the key in question. Reads within each sorted
tree structure can be done in $ O(\log n) $ through a binary search.

Going through a potentially large amount of SS-tables on disk is costly however,
and RocksDB employs a series of tricks to avoid doing so.

\subsection{Compactions}\label{sec:compactions}
To maintain immutability, new SS-tables are always created without modifying
existing on-disk content. Two writes to the same key can thus co-exist in
different SS-tables, even if only the last written key is relevant to the
system. This is quite wasteful, and would overtime lead to worse and worse read
performance. The original LSM-paper~\cite{lsm} solves this through
\textit{merging} existing LSM-trees into new ones at regular intervals. RocksDB
does so in background threads, where it is referred to as \textit{compaction}.

During compacting, multiple SS-tables are merge-sorted into a single new
structure. This process also removes duplicate keys, retaining only the last
value for future use. Tombstones are also filtered out, together with any values
they might have deleted.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{level_compaction}
  \caption{SS-tables from initial levels are compacted into the next~\cite{rocksdb-compaction}.}
\end{figure}

In RocksDB, compactions are triggered when the previous \textit{level} reaches a
certain size. Referred to as \textit{leveled compaction}, this was one of the
original contributions of LevelDB\@. As described in~\cite{rocksdb-compaction},
this is usually initiated when the amount of SS-tables at the first level, level
0, goes beyond a certain amount. This in turn might cause the next level to go
beyond its size limit, resulting in a compaction to the next level again, and so
on. Unlike LevelDB, RocksDB also supports doing compactions in parallel, as long
as there are enough available background threads to do so.

\subsection{Bloom filters}\label{sec:bloom}
Iterating through every SS-table available to find a single key is inefficient.
Instead, we would like to ask the question ``can this key possibly exist here?''
for each of the SS-tables we go through, and only operate on the ones where the
answer is ``yes''. With a regular hash based data structure this would be quite
costly in terms of space, as we would need to maintain such a structure for
every SS-table in our database. Instead, RocksDB, and many other systems like
it, rely on a probabilistic data structure known as a bloom filter~\cite{bloom}
to do so.

Instead of knowing with 100\% certainty whether a key exists in a set, a bloom
filter would let us know if that key \textit{might possibly} be in the set, or
if it is \textit{definitely} not. The third option, of possibly \textit{not}
being in the set is impossible. The positive trade-off here is that it uses
significantly less space, opening the possibility of using it for every SS-table
in the system.

\subsection{Iteration}
One of the essential features of RocksDB compared to other key-value stores is
that its data is \textit{sorted}, and that it can be queried as such through
\textit{iterators}. This opens for a wide variety of possibilities that would
not have been feasible with a regular key-value store, such as range queries.
RocksDB supports iterating both forwards and backwards.

Similar to with reads, performing a fully ordered scan in an LSM-tree based
storage engine is far from optimal: every tree-structure, or SS-table in
RocksDB, needs to be considered, and as key ranges may overlap between different
files, sorted.

A lot of applications do not rely on completely random scans of keys however,
and only need support for ordered queries within a specific \textit{key prefix}.
Developers instruct RocksDB on how to retrieve a specific prefix from each key,
which RocksDB then internally uses to organize the data in such a manner that
iterating through keys within a \textit{specific prefix} is efficient: either by
storing bloom filters for each prefix, or by managing a hash based index
structure based on the prefix.

% TODO: might show an example to better explain prefix stuff here, or save it
% for implementation.

\subsection{Column Families}

RocksDB supports the equivalent of tables from a traditional database through
\textit{column
families}\furl{https://github.com/facebook/rocksdb/wiki/Column-Families}.
Separate column families share the same write-ahead log but have their own
MemTables and SS-tables. Maintaining the same WAL makes it possible to
atomically write across multiple column families, while keeping independent
LSM-tree components open for the possibility of configuring different column
families separately---an important difference from tables in SQL databases.

Column family support was not added until version 3.0 of RocksDB.\@ To maintain
backwards compatibility, the default API methods operate on the same column
family, ``default'', with separate methods taking in an additional column family
argument.

\subsection{Customizing the MemTable implementation}

RocksDB provides multiple implementations of its in-memory
MemTables\furl{https://github.com/facebook/rocksdb/wiki/MemTable}, which can be
changed between through \textit{factories}. Different implementations have
different advantages and disadvantages, with the default being the all around
safest choice.

\subsubsection{Skip List}

The default implementation uses a \textit{skip list}, a data structure with
comparable performance guarantees to a binary search tree---$ O(\log n) $ for
searches, insertions and deletions---but with far better support for concurrent
operations. This makes the default skip list implementation the only MemTable
factory capable of concurrent insertions. Flushing a skip list MemTable to disk
is also considerably faster compared to the other factories, with a much lower
memory overhead.

\subsubsection{Hash Skip List}

RocksDB provides two hash based MemTable factories, where keys are organized in
buckets based on their extracted \textit{prefix}. This implies that the hash
based implementations are only usable when a prefix extractor is defined, and
that they only support efficient iterations within a specific prefix. At the
same time the hash based implementations are also considerably more efficient
when that is the case, providing $ O(\log k) $ performance, where $ k $ is the
number of keys within a specific prefix (which is often quite low).

\subsubsection{Hash Linked List}

Similar to the skip list based hash table, RocksDB also provides a hash based
implementation where each bucket is maintained as a linked list instead of a
skip list. This is similar to a traditional hash table with chaining as its
collision resolution, and maintains close to constant time performance
guarantees as long as the elements in each bucket is kept low. This comes with
significantly lower memory overhead compared to the skip list based hash table,
but with naturally lower performance when the amount of keys per prefix starts
to grow. Because of this the buckets in a \code{HashLinkList} are implicitly converted
to a skip list when its element count exceeds a certain threshold (256 by
default).

\subsubsection{Vector}

Finally, RocksDB also provides a MemTable factory heavily tuned for random
insertions, with abysmal performance for everything else. This makes it only
useful for bulk loading data as fast as possible.

\subsection{Customizing the SS-table implementation}

The default SS-table implementation is based on the original format from
LevelDB, \code{BlockBasedTable}. As the name implies, data is stored in separate
blocks, where each file's initial block is a filter on the rest of the contents.

\todo{write more here}

RocksDB also provides an improved format designed for low query latency on
modern storage media, \code{PlainTable}.

\subsection{RocksDB from Rust}\label{sec:rust-rocksdb}
While RocksDB is written in C++ it provides a separate API through its
C-bindings, which are used to call into it from a variety of different
languages\furl{https://github.com/facebook/rocksdb/blob/master/LANGUAGE-BINDINGS.md}.

% TODO: ref to appendix?
% TODO: ref to ffi section
This thesis makes use of a modified version of
\textbf{rust-rocksdb}\furl{https://github.com/spacejam/rust-rocksdb}, which exposes a Rust-friendly
API that eventually calls into the C-bindings.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let db = DB::open_default("db_path").unwrap();

let key = b"key";
let value = b"value";
db.put(key, value).unwrap();

match db.get(key) {
  Ok(v) => assert_eq!(*v.unwrap(), value),
  Err(e) => panic!("failed reading from rocksdb: {}", e),
}
  \end{minted}

  \caption{Simple example usage of rust-rocksdb}\label{lst:rocksdb-rust}
\end{listing}
