\chapter{Background}\label{chap:background}

\section{Soup}

\subsection{MySQL Shim}

\section{Rust}
\subsection{Foreign Function Interface}

\section{Recovery}

\section{bincode}

bincode~\cite{bincode} is a Rust binary serialization library, used heavily
throughout both this thesis and in Soup, for everything from RPC
communication to persisting data to durable storage. In short, bincode takes an
arbitrary Rust object and turns it into a series of bytes---an encoded object.
The size of the resulting byte stream is usually either less than, or the same
as, the size of the source object. bincode builds on top of the
Serde\furl{https://serde.rs/} serialization framework.

As the encoded format is of relevance to later sections, we will briefly go
through it here. Primitive values, such as numbers, are encoded directly using
Rust's \code{Writer} trait, with a few exceptions:

\begin{itemize}
  \item \code{isize} and \code{usize} types, which have varying sizes depending
    on the OS, are encoded as \code{i32} and \code{u64} correspondingly.
  \item Strings are encoded as the tuple \code{(number of bytes, bytes)}, where
    the former is a \code{u64} and the latter is a byte slice.
\end{itemize}

Compound types---enums, structs, vectors, and tuples---are encoded recursively,
with each of their fields placed out in succession. With vector lengths not
being determined at compile time, vectors are prefixed with a length field on
the form of a \code{u64}. This is not necessary for the other compound types, as
their sizes do not vary at runtime. An enum instance can represent multiple
types, and is prefixed with a \code{u32} tag used to determine which
enum variant it represents.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
#[derive(Serialize, Deserialize, Debug, PartialEq)]
enum Number {
    Positive(u64),
    Negative(u64)
}

fn main() {
    let values = vec![Number::Positive(3), Number::Negative(4)];
    // This serializes as:
    // vector length u64,
    // + enum variant u32 + u64,
    // + enum variant u32 + u64
    // = u64, u32, u64, u32, u64
    // = 32 bytes
    let raw = bincode::serialize(&values).unwrap();
    let deserialized: Vec<_> = bincode::deserialize(&raw).unwrap();

    for (i, element) in values.into_iter().enumerate() {
        assert_eq!(element, deserialized[i]);
    }
}
  \end{minted}

  \caption{}\label{lst:bincode}
\end{listing}

\section{Rust}

\subsection{Foreign Function Interface}

Rust has excellent support for calling into external C programs.

\section{Profiling}

\subsection{CPU}

A large part of application performance tuning comes down to figuring out which
portion of a program is running slowly and why that is the case. Throughout
this thesis that is accomplished using
\code{perf}\furl{https://perf.wiki.kernel.org}---a profiling tool that helps us
answer the question ``What is the CPU spending time on?''.

\code{perf} collects information from both hardware counters and logical
tracepoints. The latter is especially useful for recording call graphs of a
program, which in turn lets us produce flame graphs like the one in
figure~\ref{fig:flame-example} using tools such as
\code{FlameGraph}\furl{https://github.com/brendangregg/FlameGraph} and
\code{FlameScope}\furl{https://github.com/Netflix/flamescope}. Flame graphs show
time spent on the horizontal axis, while showing the call graph vertically.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{flame-example}
  \caption{\
    An example flame graph from events recorded using \code{perf}.
  }\label{fig:flame-example}
\end{figure}

\subsection{Memory}

Memory leaks happen when we continually allocate memory without freeing it. This
can easily happen in languages without dynamic memory allocation, when a
programmer forgets to deallocate some portion of memory after using it. At the
same time it can also happen in garbage collected languages, \eg when
continuously attaching listener functions to an event system without regard for
previous subscriptions. Rust's ownership system largely prevents issues of the
first kind from happening---variables that go out of scope are deallocated
automatically. Regardless, Rust supports calling into arbitrary C-programs (see
section~\ref{sec:ffi}), where anything could happen.

To profile memory leaks, the Valgrind
Massif\furl{http://valgrind.org/docs/manual/ms-manual.html} heap profiler is
used. Massif continuously takes snapshots of the heap, recording what memory is
used for, and where that memory was allocated from. While this is subject to
change in the future, current versions of Rust use the
\code{jemalloc}\furl{http://jemalloc.net/} memory allocator instead of the
system's default allocator. Unfortunately, memory profiling using Valgrind does
not work well with \code{jemalloc}. To resolve this we can instead force Rust to
use the system allocator, at least while profiling.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
#![feature(alloc_system)]
#![feature(global_allocator, allocator_api)]

#[global_allocator]
static ALLOC: std::alloc::System = std::alloc::System;
  \end{minted}

  \caption{Forcing Rust to use the system memory allocator makes it possible to
  profile it using tools such as Valgrind Massif.}\label{lst:system-alloc}
\end{listing}

\section{SQLite}\label{sec:sqlite}
SQLite~\cite{sqlite} is by far the most widely deployed database ever written.
Used in everything from smart phones to cars, with an estimated user count in
the magnitude of multiple billion users, SQLite is everywhere\footnote{Who
uses SQLite? \url{https://www.sqlite.org/mostdeployed.html}}. SQlite is an
embedded database, and requires no extra processes, or even threads, to run.

In a world of unreliable software, SQLite is stable as a rock. It has 100\%
branch test coverage, with a test suite containing millions of different test
cases. SQLite is, and always has been, available in the public domain. As the
name implies, SQLite provides an SQL interface to developers, with decent support
for everything from indices to views. The library itself is written in about 130
thousand lines of C code.

While the main usage of SQLite is as a persistent application store (\eg in
browsers and mobile applications), SQLite is also popularly used as an engine in
other databases. One such example is the recently open-sourced
FoundationDB~\cite{foundation}, which provides a distributed database with full
ACID transactions, where each shard makes use of SQLite at its core.

\subsection{B-trees}\label{sec:btree}
Similar to a significant amount of other relational databases, SQLite makes use
of B-trees~\cite{btree} for its on-disk index structures. This is with good
reason: B-trees are well suited for mediums that perform better with larger
blocks of data, such as traditional spinning hard drives. While it has never
been officially decided what the B in B-tree stands for, a B-tree is a
self-balancing binary tree data structure.

Unlike other tree structures, such as binary search trees, each node in a B-tree
holds multiple values. By keeping the amount of values in a node---the node
size---close to the size of a block on disk, most of a B-tree's operations can
be performed in $ O(\log_b n) $ disk reads, where $ b $ is the maximum number of
entries per block, and $ \log_b n $ the height of the tree. With traditional
storage mediums, where a single disk seek might take multiple milliseconds, this
is extremely important.

\todo{Include a figure of a B-tree and a B+tree perhaps?}

When the term B-tree is used in database systems today, it is usually used to
refer to an improved version of the traditional data structure, and known as a
B+-tree. Whereas the former stores values in all levels of the tree, the
specialized version only does so at the leaf level, with the internal nodes only
containing copies of the keys. Actual records can then be stored in a different
on-disk data structure, with pointers from the leaf nodes, and by introducing
sibling pointers at the leaf node level, range queries can be efficiently
executed by walking the bottom of the tree horizontally.

\subsection{Rollback Journal}\label{sec:sqlite-locks}
SQLite implements support for atomic transactions through the use of a rollback
journal. A historic copy of values prior to changes are kept in a separate
file---the rollback journal---so that they can be copied back to the actual
database file in the event of a \code{ROLLBACK}. Similarly, this file can be
deleted after a \code{COMMIT} of the transaction.

With a rollback journal, SQLite requires a full exclusive lock to be held for
the duration of all mutations to prevent file corruption, blocking any potential
readers from accessing the database. This is the main reason SQLite is commonly
not used as the storage system for applications that require high-performance
concurrent access to their database (\eg web applications backends with multiple
active users): only a single write operation could be performed at the time.
This is not the case with reads, which hold shared locks.

\subsection{Write-ahead Log}\label{sec:sqlite-wal}
In version 3.7.0, SQLite introduced an alternative to the traditional rollback
journal: the write-ahead log~\cite{sqlite-wal}. Maintaining the same atomicity
and durability guarantees, the use of a WAL significantly improves write
performance by catering to more sequential disk access. Additionally, reading
can now co-exist with writing, as writers do no longer block read access.

While the original rollback journal format writes directly to the database file,
maintaining old values in the rollback journal, SQLite in WAL-mode does the
opposite. Updates are appended to the WAL, and copied over to the main database
file when a \textit{checkpoint} is taken. This is also the reason readers can
continue to access the database while writes are happening, as the database file
itself is not mutated, only the WAL.\@

This introduces a slight performance penalty for reads however, as there are now
potentially two sources of truth for all content: the main database file, and
the WAL until a checkpoint happens. The longer the WAL is, the more time has to
be spent searching through it by reads.

\subsection{Interacting with SQLite}
Most applications interact with SQLite through its C-API, compiling SQL
queries into prepared binary statements, which can then be executed efficiently
with different arguments, as shown in listing \ref{lst:sqlite}.

\begin{listing}[H]
  \begin{minted}[frame=lines]{c}
sqlite3 *db;
sqlite3_stmt *statement;
char *err_msg = 0;
sqlite3_open("test.db", &db);
sqlite3_exec(
  db,
  "CREATE TABLE data (id INTEGER PRIMARY KEY)",
  NULL,
  NULL,
  &err_msg
);

// Compile a prepared statement:
sqlite3_prepare_v2(
  db,
  "INSERT INTO data VALUES (?1)",
  -1,
  &statement,
  0
);

// Then insert a single row with the value 10:
int id = 10;
sqlite3_bind_int(statement, 1, id);
  \end{minted}

  \caption{Simple SQLite C-example showing how to write a single row (error
  handling ignored for brevity)}\label{lst:sqlite}
\end{listing}

In addition, most programming languages have at least one popular library for
accessing SQLite, abstracting away the need to directly call into the C-bindings
through more idiomatic APIs for each language.

SQLite also provides a command-line interface, which can be used to directly
read from and modify a database with SQL statements.

\subsection{SQLite from Rust}
Accessing SQLite from Rust can be done through the excellent \code{Rusqlite}
library~\cite{rusqlite}, which
provides a Rust API on top of SQLite's C-bindings.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let conn = Connection::open("test.db").unwrap();
conn.execute(
  "CREATE TABLE data (id INTEGER PRIMARY KEY)",
  &[],
).unwrap();

// Compile a prepared statement:
let statement = conn.prepare("INSERT INTO data VALUES (?1)").unwrap();

// Then insert a single row with the value 10:
let id = 10;
statement.execute(&[&id]).unwrap();

  \end{minted}

  \caption{SQLite example using \code{rusqlite} showing how to write a single
  row.}\label{lst:sqlite-rust}
\end{listing}

\section{RocksDB}\label{sec:rocksdb}
RocksDB is an embedded key-value store optimized for modern flash storage.
RocksDB started out at Facebook, with the goal of making a version of Google's
LevelDB that performed well on modern hardware. Today, RocksDB is used at the
heart of a wide variety of databases, such as CockroachDB~\cite{cockroach},
MyRocks~\cite{myrocks} (Facebook's fork of MySQL) and TiDB~\cite{tidb}.

Traditional B-tree based database systems are often faced with poor write
performance as a result of random writes, which perform worse than sequential
writes on both magnetic and flash based storage mediums. RocksDB, on the other
hand, achieves impressive write performance through the use of immutable
log-structured merge trees~\cite{lsm} (LSM-trees), avoiding the need for random
writes to persistent storage altogether.

Writes are initially only written to a persistent write-ahead log (WAL) and
in-memory data structures referred to as memtables. Later these memtables are
flushed to their equivalent data structures on disk, Static Sorted Tables (SST).
The latter is done by background threads, allowing regular processing to
continue without getting backed by slow writes to persistent storage. Both of
these components originate in Patrick O'Neil's original paper on LSM-trees,
where the in-memory data structure is referred to as $ C_0 $, and the on-disk
structures $ C_{1..n} $.

% TODO:
% * Calling into RocksDB (C-API, rust-rocksdb)
% * Iterators
% * Prefix Iteration/Slice Transform

\subsection{MemTables}
All writes are initially synchronously written to an in-memory data structure---a
memtable---which is later flushed to disk at the point of filling up. Both the
size and the number of memtables can be configured at runtime.

RocksDB's default memtable implementation is a skiplist, with an $ O(\log n) $
bound on inserts, searches and deletes. This can be changed to a series of hash
based implementations, which offer better performance if all operations are done
within a pre-specified key prefix.

\subsection{Static Sorted Tables}
After a memtable reaches a certain size, RocksDB's background threads takes over
and flushes it to persistent storage. This will generate one or more SS-tables
on disk, where each file is sorted. SS-tables are immutable: a new SS-table is
always created, and existing ones are never updated. This ensures that writes
remain sequential.

\subsection{Write-ahead Log}\label{sec:rocksdb-wal}
RocksDB achieves durability through the use of a write-ahead log (WAL). Without
it, data in memtables would be lost at the event of a crash. By default, every
\code{Put} operation results in a write to the RocksDB WAL, with the optional
possibility of waiting for the write to be fully synchronized to the WAL before
returning.

Each memtable corresponds to a WAL-file, which is marked as obsolete obsolete
when the memtable has been safely persisted to disk. Each WAL-file includes a
sequence number, and the files are iterated through in order during recovery.
The WAL itself is built up of a sequence of records, where each record includes
a cyclic redundancy check (CRC) computed hash over the payload to maintain
integrity~\cite{rocksdb-wal}.

Optionally, the WAL can be written to a different directory than the regular
database files. This is essential for production systems that want to maintain a
high write throughput: compactions and memtable flushes can then utilize the
full disk capacity without slowing down the throughput of WAL writes. Even more
drastically, the database files could be written to faster, volatile storage,
relying solely on never-archived WAL-files for (albeit much slower) recovery.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let batch = WriteBatch::default()
batch.put("a", "1");
batch.put("b", "2");

let opts = WriteOptions::default();
opts.set_sync(true);
db.write(batch, &opts);
  \end{minted}

  \caption{Rust code for safely persisting a batch of writes to RocksDB and its
  write-ahead log.}\label{lst:write-batch}
\end{listing}

\code{Put} operations can also be batched into a \code{WriteBatch} (as shown in
listing~\ref{lst:write-batch}), to amortize the cost of synchronizing the WAL
over a larger amount of write operations. This is an atomic operation: either
all the writes in the write batch succeeds, or none do.

\subsection{Basic Operations}
Akin to other key-value databases, RocksDB offers a familiar API of
\code{Put(key, value)}, \code{Get(key)} and \code{Delete(key)}, operating
directly on bytestream values. Both insertions and deletions are purely
sequential: subsequent \code{Put} operations of the same key never backtrack and
overwrite existing keys, and deletions insert tombstone markers to avoid having
to randomly read and mutate previously written values.

Whereas both memtables and SS-tables are sorted, each tree structure might
overlap with another. This is a result of the immutability property, and newly
created SS-tables might contain key ranges already included in existing
structures. This means that read operations in RocksDB, and other LSM-tree based
storage systems, have to iterate through each tree structure---starting with the
memtables---in an attempt to find the key in question. Reads within each sorted
tree structure can be done in $ O(\log n) $ through a binary search.

Going through a potentially large amount of SS-tables on disk is costly however,
and RocksDB employs a series of tricks to avoid doing so.

\subsection{Compactions}\label{sec:compactions}
To maintain immutability, new SS-tables are always created without modifying
existing on-disk content. Two writes to the same key can thus co-exist in
different SS-tables, even if only the last written key is relevant to the
system. This is quite wasteful, and would overtime lead to worse and worse read
performance. The original LSM-paper~\cite{lsm} solves this through
\textit{merging} existing LSM-trees into new ones at regular intervals. RocksDB
does so in background threads, where it is referred to as \textit{compaction}.

During compacting, multiple SS-tables are merge-sorted into a single new
structure. This process also removes duplicate keys, retaining only the last
value for future use. Tombstones are also filtered out, together with any values
they might have deleted.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.5\textwidth]{level_compaction}
  \caption{SS-tables from initial levels are compacted into the next~\cite{rocksdb-compaction}.}
\end{figure}

In RocksDB, compactions are triggered when the previous \textit{level} reaches a
certain size. Referred to as \textit{leveled compaction}, this was one of the
original contributions of LevelDB\@. As described in~\cite{rocksdb-compaction},
this is usually initiated when the amount of SS-tables at the first level, level
0, goes beyond a certain amount. This in turn might cause the next level to go
beyond its size limit, resulting in a compaction to the next level again, and so
on. Unlike LevelDB, RocksDB also supports doing compactions in parallel, as long
as there are enough available background threads to do so.

\subsection{Bloom filters}\label{sec:bloom}
Iterating through every SS-table available to find a single key is inefficient.
Instead, we would like to ask the question ``can this key possibly exist here?''
for each of the SS-tables we go through, and only operate on the ones where the
answer is ``yes''. With a regular hash based data structure this would be quite
costly in terms of space, as we would need to maintain such a structure for
every SS-table in our database. Instead, RocksDB, and many other systems like
it, rely on a probabilistic data structure known as a bloom filter~\cite{bloom}
to do so.

Instead of knowing with 100\% certainty whether a key exists in a set, a bloom
filter would let us know if that key \textit{might possibly} be in the set, or
if it is \textit{definitely} not. The third option, of possibly \textit{not}
being in the set is impossible. The positive trade-off here is that it uses
significantly less space, opening the possibility of using it for every SS-table
in the system.

\subsection{Iteration}
One of the essential features of RocksDB compared to other key-value stores is
that its data is \textit{sorted}, and that it can be queried as such through
\textit{iterators}. This opens for a wide variety of possibilities that would
not have been feasible with a regular key-value store, such as range queries.
RocksDB supports iterating both forwards and backwards.

Similar to with reads, performing a fully ordered scan in an LSM-tree based
storage engine is far from optimal: every tree-structure, or SS-table in
RocksDB, needs to be considered, and as key ranges may overlap between different
files, sorted.

A lot of applications do not rely on completely random scans of keys however,
and only need support for ordered queries within a specific \textit{key prefix}.
Developers instruct RocksDB on how to retrieve a specific prefix from each key,
which RocksDB then internally uses to organize the data in such a manner that
iterating through keys within a \textit{specific prefix} is efficient: either by
storing bloom filters for each prefix, or by managing a hash based index
structure based on the prefix.

% TODO: might show an example to better explain prefix stuff here, or save it
% for implementation.

\subsection{Column Families}

RocksDB supports the equivalent of tables from a traditional database through
\textit{column
families}\furl{https://github.com/facebook/rocksdb/wiki/Column-Families}.
Separate column families share the same write-ahead log but have their own
MemTables and SS-tables. Maintaining the same WAL makes it possible to
atomically write across multiple column families, while keeping independent
LSM-tree components open for the possibility of configuring different column
families separately---an important difference from tables in SQL databases.

Column family support was not added until version 3.0 of RocksDB.\@ To maintain
backwards compatibility, the default API methods operate on the same column
family, ``default'', with separate methods taking in an additional column family
argument.

\subsection{Different MemTable implementations}

RocksDB provides multiple implementations of its in-memory
MemTables\furl{https://github.com/facebook/rocksdb/wiki/MemTable}, which can be
changed between through \textit{factories}. Different implementations have
different advantages and disadvantages, with the default being the all around
safest choice.

\subsubsection{Skip List}

The default implementation uses a \textit{skip list}, a data structure with
comparable performance guarantees to a binary search tree---$ O(\log n) $ for
searches, insertions and deletions---but with far better support for concurrent
operations. This makes the default skip list implementation the only MemTable
factory capable of concurrent insertions. Flushing a skip list MemTable to disk
is also considerably faster compared to the other factories, with a much lower
memory overhead.

\subsubsection{Hash Skip List}

RocksDB provides two hash based MemTable factories, where keys are organized in
buckets based on their extracted \textit{prefix}. This implies that the hash
based implementations are only usable when a prefix extractor is defined, and
that they only support efficient iterations within a specific prefix. At the
same time the hash based implementations are also considerably more efficient
when that is the case, providing $ O(\log k) $ performance, where $ k $ is the
number of keys within a specific prefix (which is often quite low).

\subsubsection{Hash Linked List}

Similar to the skip list based hash table, RocksDB also provides a hash based
implementation where each bucket is maintained as a linked list instead of a
skip list. This is similar to a traditional hash table with chaining as its
collision resolution, and maintains close to constant time performance
guarantees as long as the elements in each bucket is kept low. This comes with
significantly lower memory overhead compared to the skip list based hash table,
but with naturally lower performance when the amount of keys per prefix starts
to grow. Because of this the buckets in a \code{HashLinkList} are implicitly converted
to a skip list when its element count exceeds a certain threshold (256 by
default).

\subsubsection{Vector}

Finally, RocksDB also provides a MemTable factory heavily tuned for random
insertions, with abysmal performance for everything else. This makes it only
useful for bulk loading data as fast as possible.

\subsection{RocksDB from Rust}
While RocksDB is written in C++ it provides a separate API through its
C-bindings, which are used to call into it from a variety of different
languages\furl{https://github.com/facebook/rocksdb/blob/master/LANGUAGE-BINDINGS.md}.

% TODO: ref to appendix?
% TODO: ref to ffi section
This thesis makes use of a modified version of
\textbf{rust-rocksdb}\furl{https://github.com/spacejam/rust-rocksdb}, which exposes a Rust-friendly
API that eventually calls into the C-bindings.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
let db = DB::open_default("db_path").unwrap();

let key = b"key";
let value = b"value";
db.put(key, value).unwrap();

match db.get(key) {
  Ok(v) => assert_eq!(*v.unwrap(), value),
  Err(e) => panic!("failed reading from rocksdb: {}", e),
}
  \end{minted}

  \caption{Simple example usage of rust-rocksdb}\label{lst:rocksdb-rust}
\end{listing}
