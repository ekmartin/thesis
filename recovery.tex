\chapter{Recovery}\label{chap:recovery}

This chapter describes the current recovery scheme used in Soup, followed by a
look at how it improves when the base tables are stored on durable storage
instead of in volatile main-memory. Finally, the rest of the chapter is reserved
for snapshotting---a significant improvement to Soup's recovery capabilities.

\newpage

\section{Write-ahead log}

Soup appends all updates to a durable write-ahead log prior to sending out write
acknowledgments. Packets are buffered up using a group-commit
scheme~\cite{main-memory} to amortize the I/O cost of writing to durable
storage. Commits result in a merged packet injected into the data-flow graph,
ensuring that all changes are persisted to disk before being accessed by
readers.

Merging the data from multiple packets before writing to the WAL drastically
increases the throughput of the system. Instead of requiring a single write to
durable storage per packet, Soup only needs to write once per batch. At the same
time, it reduces the total amount of packets fed through the data-flow graph,
allowing each node to operate on a larger batch of records at a time.

\subsection{Log based recovery}\label{sec:log-recovery}

In a traditional relational database management system, log recovery often
involves multiple phases. Following a system like ARIES (described in
section~\ref{sec:recovery}), log recovery is responsible for a wide variety of
tasks, such as undoing aborted transactions. Log recovery in Soup is much
simpler, with log entries only including the operations to be performed, and not
the result they have on the database---a \textit{command
log}~\cite{voltdb-recovery}. There are no \code{UNDO} nor \code{REDO} phases
either: only committed transactions result in persisted log entries.

Log recovery is then a matter of replaying the writes from the persisted logs by
feeding them into the data-flow graph's base nodes as if they were regular
updates. Each line in the log corresponds to a packet that was once injected
into the Soup instance's data-flow graph, now serialized using JSON.\@

\begin{listing}[H]
  \begin{minted}{json}
[
  [{"Positive": [{"Int": 1}, {"Int": 0}]}],
  [{"Positive": [{"Int": 1}, {"Int": 1}]}],
  [{"Positive": [{"Int": 1}, {"Int": 2}]}]
]
  \end{minted}
  \caption{\
    An expanded line from one of the log files of a Soup application,
    corresponding to a single batched update with three records.
  }\label{lst:log-entry}
\end{listing}

Log entries are separated by a newline character, where each line in the log is
valid JSON.\@ This lets the recovery process handle one line at a time in a
buffered fashion, avoiding the need to store the entire log in memory. Similar
to how the packets were created in the first place, individual updates from each
log entry are batched into chunks, before finally being turned into separate
packets that are passed on to the rest of the data-flow graph. This speeds up
recovery performance by avoiding processing of a potentially large amount of
small log entries.

\begin{figure}[H]
  \includesvg[width=\textwidth]{log-chunking}
  \caption{\
    Log entries are flattened, then chunked into individual packets.
  }\label{fig:log-chunking}
\end{figure}

\section{Persistent base nodes}

RocksDB recovers content that resided in its in-memory MemTables at the time of
a crash by replaying entries from its write-ahead log. This is a stark
improvement from recovery using Soup's regular write-ahead log, where every
entry from the beginning of time has to be replayed. Data that is already
persisted to durable SS-tables at the time of a crash require no extra
work---they can be read in the same manner after crashing as before.

What happens to Soup's partially materialized nodes? Similar to a regular
caching solution, they start out completely empty after recovering. Subsequent
requests gradually restore the nodes to a state resembling the one they were in
before crashing. Fully materialized nodes require a complete view of the state
at all times and need to be sent a full copy of the state when recovering.

Both this, and the fact that partially materialized nodes start out empty, lead
to reduced initial performance---a target of improvement addressed in the next
section.

\section{Snapshotting}\label{sec:snapshotting}

While \code{PersistentState} speeds up base node recovery, it comes with no
improvement for recovery of nodes further down the graph. Partial nodes have to
trigger a large amount of replays to restore their state early on, while fully
materialized nodes need a complete copy of the state altogether before serving
any reads at all.

\code{PersistentState} lets base nodes instantly recover to a recent point in
time, capping recovery to the time it takes to go through recent updates. A
similar solution for all materialized state would let nodes recover to a recent
checkpoint, followed by re-application of log entries to become fully
up-to-date. In short, we need to be able to consistently \textit{snapshot} the
materialized nodes at any given point.

The implementation in the rest of this section is based on an earlier version of
this thesis~\cite{project}, submitted as a part of ``TDT4501 Computer Science,
Specialization Project''.

\subsection{Challenges}

Main memory systems like VoltDB leverage checkpointing by persisting the
transactional state of committed transactions, using log sequence numbers to be
able to track which updates have been reflected on disk~\cite{voltdb-recovery}.
In Soup, state is materialized at a variety of nodes throughout the graph, and
updates have no timestamps or sequence numbers attached to them. Updates
propagate through the graph asynchronously, and a specific update is likely to
reach different points in the graph at separate times. Taking a global snapshot
of the entire graph simultaneously would mean capturing nodes at different
logical points, as an update might be in the process of propagating throughout
the graph at the time that the snapshot is initiated.

Soup's way of asynchronously propagating updates through its data-flow graph
resembles the communication done in a distributed system. Being able to observe
the global state in a distributed system---where access to a common clock is
rare---is an immensely useful property, crucial to resolving a certain category
of problems, such as deadlock detection and global checkpointing.
Section~\ref{sec:rel-snapshotting} introduces a series of algorithms for
snapshotting distributed systems, including the
Chandy-Lamport~\cite{chandy-lamport} method of propagating an explicit snapshot
marker throughout the system to ensure consistency, which we implement later in
this chapter.

\subsection{Algorithm}

Taking a snapshot of a running Soup instance requires persisting the content of
each materialized node in the current data-flow graph. This needs to happen at
the same logical point in time across the graph---ensuring that every in-flight
update is either propagated to \textit{all} nodes in the graph---or none of
them, leading to a consistent state after recovering from a failure. At the same
time, taking a snapshot should not incur a too heavy performance cost on the
running system and should definitely not stop the system from processing updates
completely---for any period of time. This lets us derive a few base rules for
our snapshotting algorithm:

\begin{enumerate}
  \item Snapshots need to include exactly the same updates across the graph.
  \item Snapshots should not significantly degrade the system's throughput.
  \item Snapshots should complete in a reasonable amount of time.
\end{enumerate}

We can then use these rules to build a snapshotting algorithm in incremental
steps, starting from an example that fails to meet the defined criteria.
Figure~\ref{fig:bad-example} shows an update propagating through the Soup
data-flow graph. What would be the outcome if both of the partially materialized
nodes---shown in a blue color---would snapshot their state at the exact moment
shown in the graph? Whereas the leftmost domain has had time to process update
\code{A}, the rightmost one has not. The two domains are at different
\textit{logical} points in time, and the snapshots would fail our first rule.

What if the system as a whole instead waited for the update to completely
propagate through the graph before initiating the snapshot? This would
successfully follow the first rule, but fail the second: no new updates could be
served until the snapshot has completed across the graph, halting
the system's throughput.

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{bad-example}
  \caption{\
    An update \texttt{X} propagates through the domains in the graph in an
    asynchronous manner. Domains $ B $ and $ C $ contain at least one
    materialized node and should be snapshotted.
  }\label{fig:bad-example}
\end{figure}

\subsubsection{Synchronous snapshotting}

Soup's data-flow graph forwards updates over ordered FIFO channels, making it
possible to rely on Chandy-Lamport's marker technique to determine which updates
should be considered a part of a snapshot. Domains that receive the
marker initiate the snapshot process right away, without any further processing
of updates. This results in a global snapshot taken at the same \textit{logical}
point in time, even if the actual snapshots were instantiated at different
\textit{physical points}.

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{good-example}
  \caption{\
    An update \texttt{X} is propagated through the data-flow graph, followed by
    a snapshot marker to ensure that domains with materialized nodes snapshot at
    the correct time.
  }\label{good-example}
\end{figure}

The controller initiates a snapshot by issuing a \code{TakeSnapshot} marker to
each of its base nodes, which is then propagated through the rest of the graph.
After a snapshot completes, the controller persists its \code{snapshot\_id} to
durable storage, to ensure that it can inform domains which snapshot to recover
from after a failure.

\begin{listing}[H]
  \begin{minted}[frame=lines]{python}
InitializeSnapshot:
  snapshot_id += 1
  for node in base_nodes:
    node.send(TakeSnapshot, snapshot_id)

  for node in base_nodes:
    node.wait_for_ack()

  persist(snapshot_id)
  \end{minted}

  \caption{Initiating a snapshot from the controller.}
\end{listing}

Domains that receive a snapshot marker proceed with the snapshotting process
immediately. After persisting the snapshot, nodes notify the controller that
they have done so, letting it eventually discard log entries after confirming
that all materialized nodes have successfully taken a snapshot of their state.
Implemented naively, this would involve serializing each node's state at a
domain and persisting these to disk---all while while blocking updates from the
rest of the system (shown in listing~\ref{lst:naive}).


\begin{listing}[H]
  \begin{minted}[frame=lines]{python}
TakeSnapshot:
  for node in nodes:
    if node.is_materialized:
      state = serialize(node.state)
      write(state)
      notify_controller(snapshot_id)
  \end{minted}
  \caption{\
    The beginning of a snapshot implementations for domains.
  }\label{lst:naive}
\end{listing}


\subsubsection{Asynchronous snapshotting}\label{sec:async-snapshot}

Our synchronous snapshotting algorithm fulfills the first snapshotting base rule
through use of Chandy-Lamport's marker technique, by stopping further domain
processing until the snapshot has completed. This involves writing the snapshot
to durable storage---a potentially slow operation. To prevent this from
significantly slowing down the system's throughput we would need to persist the
snapshot in a separate computational unit, such as a thread, allowing the
domain to continue its regular processing without pause.

The algorithm shown in listing~\ref{lst:snapshot-worker} achieves this through a
\code{SnapshotWorker} running in a separate thread, where the received state is
serialized and persisted to durable storage before the controller is notified of
its completion.

\begin{listing}[H]
  \begin{minted}[frame=lines]{python}
SnapshotWorker:
  for event in listener:
      state = serialize(event.state)
      write(state)
      notify_controller(event.snapshot_id, event.state)

TakeSnapshotAsync:
  states = {}
  for node in nodes:
    if node.is_materialized:
      states[node] = node.state.clone()

  snapshot_worker.send(snapshot_id, states)
  \end{minted}
  \caption{\
    A \code{SnapshotWorker} serializes and persists snapshot in a thread
    separate from the regular domain processing.
  }\label{lst:snapshot-worker}
\end{listing}

\subsubsection{Delayed snapshotting}

Compared to the synchronous snapshotting algorithm---where processing is
parallelized across all available domains---our asynchronous version restricts
the number of parallel units to a fixed set of snapshotting workers. This number
is likely to be far less than the number of domains, introducing a trade-off
between snapshot completion time and the extra load induced on the system. The
former is irrelevant as long as each snapshot completes before the next request
arrives, and the focus should without doubt be on avoiding a potential
performance hit to the processing throughput.

While the introduction of asynchronous snapshot workers move the bulk of the
snapshot processing out of the domains' main thread, the state clone operation
remains. This is a crucial part in maintaining the correctness of
Chandy-Lamport's marker technique, and snapshots would not happen at the correct
logical instant without it. Still, with snapshots happening roughly at the same
physical time across the graph, the pause required from cloning a domain's
entire state could have a significant impact on the system's total throughput.

\todo{should have a figure here}

Instead, we would like to amortize the performance penalty across a larger time
range, by delaying the snapshotting process at each individual domain. While
snapshots would still need to happen at the same \textit{logical} point across
the graph, the \textit{physical} instant could vary. Naturally, this could be
achieved by cloning the state immediately and only forwarding the result later
on---without any gain at all. To actually spread out the cost of snapshotting we
would need to delay the clone as well.

A clone of a domain's state has to be taken at some point, but preferably later
than when the snapshot marker arrives. This would require the ability to travel
back in time from a state $ S_m $ to the original state when the marker arrived,
$ S_n $. In short, with $ L_{n..m} $ signifying the updates that arrived from $
n $ to $ m $, $ S_n $ can be re-created through $ S_n - L_{n..m} $.
The amount of work performed by a single domain would be higher, but in return
the individual clone operations performed across all domains could be delayed,
preventing a global performance penalty.

\begin{listing}[H]
  \begin{minted}[frame=lines]{python}
TakeSnapshotDelayed:
  states = {}
  for node in nodes:
    if node.is_materialized:
      current_state = node.state.clone()
      states[node] = current_state - processed_updates[node]

  snapshot_worker.send(snapshot_id, states)
  \end{minted}
  \caption{\
    A delayed implementation of \texttt{TakeSnapshotAsync} from
    listing~\ref{lst:snapshot-worker}. Updates arriving after the marker are
    stored in \code{processed\_updates}.
  }
\end{listing}

\subsubsection{Snapshot confirmations}

The controller should be notified of all completed snapshots, as shown in
listing~\ref{lst:snapshot-ack}. The snapshots are already persisted to disk at
this point, removing the need to include data in the acknowledgment messages.
The messages should on the other hand include the current snapshot identifier,
so that the controller eventually knows when the entire graph has completed the
same snapshot. At that point the snapshot identifier can be persisted to
ZooKeeper, and any log entries for updates prior to the snapshot being taken can
be discarded. Recovery is then a matter of first loading each materialized
node's state from their local snapshot, followed by replaying the rest of the
log entries available.

\begin{listing}[H]
  \begin{minted}[frame=lines]{python}
ReceiveSnapshotAck(domain_id, snapshot_id):
  snapshot_ids[domain_id] = snapshot_id
  if min(snapshot_ids) == snapshot_id:
    persist_snapshot_id(snapshot_id)
  \end{minted}
  \caption{\
    The controller listens for snapshot acknowledgments from snapshot workers,
    updating an internal data structure with a mapping from domain to
    \code{snapshot\_id} on each received confirmation. When all domains have
    completed their snapshots, the controller persists the
    \code{snapshot\_id}, so that it later on can be used for recovery.
  }\label{lst:snapshot-ack}
\end{listing}


\subsubsection{Failure before discarding the log}

In the event of a failure \textit{after} the controller has persisted the
snapshot ID to ZooKeeper, but \textit{before} all nodes have managed to discard
the correct log entries from durable storage, duplicate replaying of those log
entries upon recovery is a possibility. To prevent this from happening, each log
entry should include their domain's current snapshot ID, so that log entries
corresponding to old snapshots can be ignored during recovery.

\subsection{Implementation}

The snapshotting implementation roughly follows along the lines of the
asynchronous snapshotting algorithm described in
section~\ref{sec:async-snapshot}. Snapshots are initiated with a specific marker
sent by the controller, domains process snapshots by cloning their nodes' state,
and snapshot workers are responsible for serializing and persisting the
snapshots. Finally, the loop is closed when the controller is notified of each
domain's completed snapshots.

\subsubsection{Initializing a snapshot}

Snapshots are initialized by the controller using a special marker packet,
\code{TakeSnapshot}. This happens at a regular interval, defined by the
configuration option \code{snapshot\_timeout}. The controller processes events
in an event loop, and snapshots are triggered by emitting an event to this
internal loop. This is done from a separate thread, which sleeps until it is
time to take a snapshot.

\begin{figure}[H]
  \begin{center}
    \includesvg[width=0.6\textwidth]{take-snapshot}
    \caption{\
      Snapshots are initialized when the controller sends a
      \texttt{TakeSnapshot} packet, which is forwarded through the data-flow
      graph by each domain.
    }\label{fig:take-snapshot}
  \end{center}
\end{figure}

When the controller's main loop receives a request to initialize a snapshot, it
first makes sure that it has received all confirmations from previous snapshots
before proceeding. Then it increments its \code{snapshot\_id} and fires off a
\code{TakeSnapshot} packet to each of the base node domains in its data-flow
graph. No further blocking is required: snapshot acknowledgments are handled
separately.

\subsubsection{Domain snapshotting}

Whenever a domain receives a \code{TakeSnapshot} packet, it immediately clones
the state of each of its materialized nodes. Each domain then forwards the
snapshotting packet to its children, ensuring that all materialized
nodes eventually get snapshotted. Finally, it sends the cloned states to its
\code{SnapshotWorker}, by issuing a \code{PersistSnapshotRequest}.

\code{TakeSnapshot} packets are forwarded to descendant domains through the
egress nodes in the data-flow graph. Each domain is connected to its children
through egress nodes and its ancestors through ingress nodes. With domains
running in separate computational units, the snapshotting process can complete
out of order at different domains.

\subsubsubsection{Nodes and readers}

The snapshotting algorithm describes persisting the state of materialized nodes.
Although that is true, reality is slightly more nuanced. Materialized nodes in
Soup can be both internal and external, where the latter is represented by
readers (see section~\ref{sec:soup-architecture}). Both the internal nodes and
the readers can be either partially or fully materialized, where only partial
nodes require support from their parent nodes to fulfill certain queries.

Whether a node is partially or fully materialized does not make much of a
difference for the snapshotting algorithm. On the other hand, internal and
external nodes require slightly different snapshotting and recovery methods.
This comes as a result of how state is stored within the two: whereas the former
use Soup's own \code{State} data structure, the latter makes use of the
\code{evmap}~\cite{evmap} library.

\subsection{Performing snapshot requests}

While each domain is responsible for gathering up the cloned copies of state
needed to eventually recover from a failure, the actual serialization and
persisting of snapshots happens in separate snapshot workers. The current
implementation utilizes one snapshot worker per worker pool, resulting in one
thread per running Soup instance. This could be increased as needed, but it is
favorable that snapshots happen over a longer time to prevent reducing the
system's throughput during the snapshotting process.

When a domain has finished cloning the state of its materialized nodes, it
notifies its local snapshot worker through an asynchronous and unbounded
buffered
\code{channel}\furl{https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html}.
This ensures that snapshot processing can continue separately, without blocking
the domain's regular workload. The snapshot workers receive and process
\code{PersistSnapshotRequest} events one by one. The processing involves
serializing the cloned state, persisting the serialized state to disk, and
finally notifying the controller of a domain's snapshot completion.

\subsection{Receiving snapshots confirmations}

The controller listens for snapshot confirmations on a TCP socket normally used
for coordination messages between the controller and its individual workers.
Whenever it receives an acknowledgment packet from a snapshot worker, it stores
the given \code{snapshot\_id} in a \code{HashMap} mapping each domain containing
at least one materialized node to their current snapshot ID.\@ Note that domains
can be sharded, so each instance in the map represents an individual shard of a
single domain.

\begin{figure}[H]
  \includesvg[width=\textwidth]{snapshot-id}
  \caption{\
    The controller keeps track of each domain shard's \texttt{snapshot\_id} as
    the snapshot confirmations arrive. When all shards have been snapshotted,
    the controller's \texttt{snapshot\_id} is persisted.
  }\label{fig:snapshot-id}
\end{figure}

When all shards have completed its snapshot, the current \code{snapshot\_id} is
persisted to ZooKeeper, allowing the next snapshot to be initiated.

\subsection{Logging and snapshotting}

To be able to discern between log entries created before and after a given
snapshot, log entries are annotated with the ID of the last snapshot persisted
for that specific node. This lets the recovery process ignore any log entries
created prior to the snapshot being restored.

\begin{listing}[H]
  \begin{minted}{json}
[0, [[{"Positive": [{"Int": 1}, {"Int": 0}]}]],
[1, [[{"Positive": [{"Int": 1}, {"Int": 1}]}]]]
[1, [[{"Positive": [{"Int": 1}, {"Int": 2}]}]]]
  \end{minted}
  \caption{\
    Separate log lines for a given base node. Each line is on the format shown
    in listing~\ref{lst:log-entry}, with the addition of a prefixed
    \code{snapshot\_id}.
  }
\end{listing}

The snapshot ID used for log annotations is the ID of the latest snapshot
initiated at that domain, regardless if the global snapshot with that ID has
finished yet or not---it is \textit{optimistic}. In the event of a failure, any
unfinished snapshots are simply ignored, and the log entries are relied on
instead. As an example, consider a domain that is in the process of taking a
snapshot with ID $ X $. A failure occurring after that domain has snapshotted,
but before the entire graph has done so, would lead to each domain recovering
the snapshot with ID $ X - 1 $, ignoring any log entries with $ snapshot\_id < X
- 1 $.

\subsection{Recovering from a snapshot}

Snapshot recovery comes as a supplement to the log based recovery described in
section~\ref{sec:log-recovery}. It is initialized when the controller sends a
\code{StartRecovery} packet to each of its base node domains, containing the ID
of the snapshot that was last persisted across the entire data-flow graph. The
ID is read from ZooKeeper, ensuring that it survives across failures.

When the base node domains receive the recovery packets, they initialize the
recovery process by first restoring snapshots, before replaying any updates that
are left from log entries. Before this, the recovery packet is forwarded to the
rest of the graph. Unlike log based replays, snapshots have to be restored at
each materialized node across the \textit{entire} graph---not solely at the base
nodes. Forwarding the recovery packet prior to replaying log entries is crucial,
as it ensures that snapshot recovery happens ahead of log based recovery for
every node---not just the base nodes.

\subsection{Serialization and deserialization of snapshots}

Snapshots are serialized representations of each node's materialized state. The
state itself is serialized from its Rust representation to a series of bytes
using the \code{bincode} library, as described in section~\ref{sec:bincode} and
shown in listing~\ref{lst:ser-snapshot}.

\begin{listing}[H]
  \begin{minted}[frame=lines]{rust}
// Implementing the Serialize and Deserialize traits makes it
// possible for bincode to serialize and deserialize the State struct:
#[derive(Clone, Serialize, Deserialize)]
pub struct State<T: Hash + Eq + Clone + 'static> {
    state: Vec<SingleState<T>>,
    by_tag: HashMap<Tag, usize>,
    rows: usize,
}

// Serialization:
let file = File::create(&filename)
  .expect(&format!("Failed creating snapshot file: {}", filename));
let mut writer = BufWriter::new(file);
bincode::serialize_into(&mut writer, &state, bincode::Infinite)
  .expect("bincode serialization of snapshot failed");

// And deserialization:
let file = File::open(&filename)
  .expect(&format!("Failed reading snapshot file: {}", filename));
let mut reader = BufReader::new(file);
bincode::deserialize_from(&mut reader, bincode::Infinite)
  .expect("bincode deserialization of snapshot failed")
  \end{minted}
  \caption{\
    State is serialized and deserialized using \texttt{bincode}~\cite{bincode}.
  }\label{lst:ser-snapshot}
\end{listing}

\subsection{Snapshot compression}

Writing to and reading from disk is a significant part of the work being done
during snapshotting and recovery. Both of these are naturally influenced by the
performance of the underlying storage medium. Compressing snapshots before they
are persisted would lower the amount of bytes being written, at the expense of
CPU cycles needed to compress the data before doing so.

Compression in Rust can be done using the \code{flate2}
library\furl{https://docs.rs/flate2/}, which supports a series of formats and
backends. By working on streams, \code{flate2} composes well with the
serialization library used for snapshotting, \code{bincode}.

\begin{listing}[H]
  \begin{minted}[frame=lines, escapeinside=||]{rust}
// Serialization:
let file = File::create(&filename)
  .expect(&format!("Failed creating snapshot file: {}", filename));
let buffered = BufWriter::new(file);
|\colorbox{lime}{let mut writer = ZlibEncoder::new(buffered, Compression::default());}|
bincode::serialize_into(&mut writer, &state, bincode::Infinite)
  .expect("bincode serialization of snapshot failed");

// And deserialization:
let file = File::open(&filename)
  .expect(&format!("Failed reading snapshot file: {}", filename));
let buffered = BufReader::new(file);
|\colorbox{lime}{let mut reader = ZlibDecoder::new(buffered);}|
bincode::deserialize_from(&mut reader, bincode::Infinite)
  .expect("bincode deserialization of snapshot failed")
  \end{minted}
  \caption{\
    Serialization and deserialization of compressed snapshots using
    \texttt{bincode} and \texttt{flate2}.
  }
\end{listing}

\subsection{Persisted data}

Soup persists data to both local files and ZooKeeper as a part of the
snapshotting and recovery process. Individual snapshots are written to durable
storage locally at each domain, while the ID of the last completed global
snapshot is stored in ZooKeeper, ensuring consensus between the replicated Soup
controllers.

\subsection{Diamonds in the data-flow graph}

A single Soup node can receive multiple snapshot markers if at least two nodes
have a common descendant in the data-flow graph. The snapshot implementation
described so far immediately snapshots when a marker is received and ignores
subsequently received snapshot markers. With packets propagating through the
data-flow graph asynchronously, snapshotting after the first marker is received
could produce inconsistent snapshots. Consider the case in
figure~\ref{fig:diamond}, where the snapshot marker from node $ B $ arrives at $
D $ ahead of the marker from $ C $. Snapshots performed when the marker from $ B
$ arrives would not include the propagated update from $ C $, while delaying the
snapshotting until the marker has arrived from $ C $ would potentially include
update $ U_Y $ in the snapshot, breaking the properties defined by the
Chandy-Lamport algorithm.

Instead, $ D $ should wait with snapshotting until it has received the marker
from both of its parents, while blocking further updates from parents that it
has already received the marker from.

\begin{figure}
  \centering
  \includesvg[width=0.8\textwidth]{diamond}
  \caption{\
    Updates propagate through the data-flow graph asynchronously. To correctly
    create a consistent snapshot, domain $ D $ should delay processing of $ U_Y
    $ until it has received the marker from both $ B $ and $ C $.
  }\label{fig:diamond}
\end{figure}
