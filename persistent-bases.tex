\chapter{Persistent Base Nodes}\label{chap:persistent-bases}

Updates begin their journey through the Soup data-flow graph at the base nodes,
after being successfully persisted to Soup's write-ahead log
(see~\ref{sec:soup-log}). While nodes further down in the graph might be
\textit{partial}, the base nodes always contain every single record a Soup
application has seen through its lifetime. This is crucial in maintaining a
balance between efficient read queries and space usage: popular queries will be
handled by partial state further down the graph, while reads for in-frequently
accessed rows will be able to refer all the way up to the source of truth, the
base nodes, through what in Soup is called a replay (see~\ref{sec:replays}). In
comparison to existing database systems, base nodes are closest to what
otherwise might be known as \textit{tables}.

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{base-nodes}
  \caption{Updates enter the base nodes after being persisted to a write-ahead
  log.}
\end{figure}

While partial nodes can use \textit{eviction} (see~\ref{sec:eviction}) to keep
their memory footprint low, the size of base nodes will continue to grow
unbounded throughout a Soup instance's lifetime. In the short term this can be
handled by sharding Soup's data across multiple machines in a cluster, however
this is infeasible in the long term: sustained write workloads would continue to
grow the base node state, regardless of whether the data is accessed by queries
or not.

To combat this we would like to to move either parts of, or all of, the state
stored in base nodes to durable storage. This would reduce Soup's overall memory
usage, and perhaps even more significantly, transition Soup from a purely
in-memory database to a system that can store more data than its available
memory. With data safely persisted to base nodes, recovery after a failure would
also require less work, as partial nodes could gradually recover when data is
requested through replays. Summarized, introducing persistence to Soup's base
nodes would achieve the following goals:

\begin{enumerate}
  \item Prevent Soup's memory usage from growing unbounded over time
  \item Support larger-than-memory data sets
  \item Reduce recovery time after failures
\end{enumerate}

\section{In-Memory State}\label{sec:in-memory-state}
The current in-memory state implementation provides a key-value API with support
for multiple indices. A separate state data structure is kept for each
materialized node, including base nodes. The same data structure is used by both
partially and fully materialized nodes, however as base nodes always have to be
fully materialized this section will omit describing details regarding the
former. The state data structure will be referred to as \code{State}.

\subsection{Adding indices}
The \code{State::add\_key} method introduces a new index to a specific \code{State} map,
and takes a set of columns as its argument. Additional indices do not lead to
multiple copies of the data, but rather contain pointers to the existing rows.
These pointers are held in separate data structures however: each new index
introduces a new hash map structure responsible for answering queries for that
specific set of columns.

New indices have to be \textbf{built}, as they are expected to answer queries
for data that has already been inserted, right away.

\begin{listing}[H]\label{lst:existing-index}
  \begin{minted}[frame=lines]{rust}
state = State()
// Initialize `state` with an index on the first column:
state.add_key([0])

// Then insert two rows:
state.insert([A, 1])
state.insert([B, 1])
assert_equal(state.lookup(A), [[A, 1]])

// Now, add an index on the second column:
state.add_key([1])
// ...which should return values that existed prior to the index being added:
assert_equal(state.lookup(1), [[A, 1], [B, 1]])
  \end{minted}

  \caption{Pseudo-code test that shows the expected behavior for adding indices
  with existing values.}
\end{listing}

\subsection{Retrieving values}
A stateful map would not be useful without a method for retrieving data, which
\code{State} provides through \code{State::lookup}. This takes a set of columns
and a key as its arguments, and make use of a \textbf{single} index to retrieve
one or more existing values. Each index is held in a separate hash map, and
retrievals can thus be completed in constant time.

That \code{State::lookup} can return more than one value is an important
distinction from most key-value stores, and is absolutely crucial in
implementing secondary indices. Without it \code{State} would only be able to
serve as an index structure for \textit{unique primary keys}.

\subsection{Inserting values}
Naturally, to be able to read anything in the first place, we first need to
insert data into our stateful data structure. This is done through the
\code{State::insert} method, which takes a single row as its argument.
\code{State::insert} is responsible for updating every index in \code{State},
and has to loop through and insert a pointer for each included index.

\subsection{Removing values}
Similar to insertions, removals have to update all indices. Contrary to what one
might expect, \code{State::remove} takes a single row as argument, and not a
key. This is due to how \code{State} is used in Soup: as an internal storage
unit for partially and fully materialized nodes alike. While it might make sense
for a base node to only delete rows by key, other nodes might need support for
deleting a single row, regardless of if its key is unique or not. No matter,
translating \code{remove(row)} to \code{remove(key)} is trivial, as shown in
listing~\ref{lst:removals}.

\begin{listing}[H]\label{lst:removals}
  \begin{minted}[frame=lines]{rust}
state = State()
state.add_key([0])
state.insert([A, 1])

row = state.lookup(A)
state.remove(row)
  \end{minted}

  \caption{Deleting a row from a base node in Soup.}
\end{listing}

\subsection{Updating values}
In a similar vein to how \code{State} does not have a method for removing by
key, \code{State} does not include a method for updating a single row. Updates
are instead handled by the base node's logic, by first emitting a negative
record---to delete the existing row---followed by a positive one to insert the
new value.

\section{Requirements}
Random access memory is, and has been for years now, fast. Durable storage is
undergoing a similar transformation with the recent introduction of NVMe
SSDs~\cite{nvme}, but is still orders of magnitudes slower than RAM.\@ This
means that while introducing persistent storage into parts of the Soup equation
comes with many benefits, increased performance is not likely to be one of them.
Rather, the goal will be to maintain the existing performance characteristics,
while still achieving our three main goals.

\subsection{Write throughput}
Backfills of missing state, replays, go through the regular update paths in the
Soup data-flow graph. An increase in processing latency at the base nodes would
not only reduce the write throughput, but also have a significant impact on
reads that require a replay to complete.

\subsection{Point query performance}
Base node queries should be a seldom occurrence, as most queries should be
served by nodes further down the graph. Still, when a query eventually makes it
all the way up to the base nodes, it is important that it can be completed fast
enough to not significantly slow down the total read throughput of the system.
Slightly higher latency is on the other hand to be expected, we are after all
comparing persistent storage to an in-memory hash map.

\subsection{Support both primary and secondary indices}
Similar to Soup's existing \code{State} implementation, a durable replacement
has to support mapping a single key to multiple values, \ie secondary indices.

\section{Embedding an existing storage engine}
On-disk data structures have wildly varying performance characteristics. A
B+tree (see~\ref{sec:btree}) might perform well on random reads, but get heavily
out-performed by an LSM-tree (see~\ref{sec:rocksdb}) on sequential writes. At
the same time, decades of changes in hardware research have broadened the field
even further. A new NVMe SSD is able to reach almost half a million random reads
per
second\footnote{\url{http://www.samsung.com/semiconductor/minisite/ssd/product/consumer/ssd960/}},
whereas a traditional spinning disk barely scratches the surface of a
hundred\footnote{\url{https://www.symantec.com/connect/articles/getting-hang-iops-v13}}.
This makes building data structures for durable storage non-trivial and time
consuming.

On the other hand, there is a plethora of existing, open-source, storage
backends available today. Similar to their underlying data structures, different
backends are built for different use cases, with different hardware in mind.
This provides an option to implementing data structures from scratch, by instead
making use of existing database systems to test performance assumptions, which
is exactly what this thesis will do: first using SQLite, and later using
RocksDB.\@

\subsection{State Interface}
Before diving into the individual \code{State} operations, we need to pave the
way for the possibility of even having two different state implementations: the
existing in-memory implementation, and the new persistent storage variant, from
here on referred to as \code{MemoryState} and \code{PersistentState}. This is
achieved by turning the existing \code{State} implementation into an
interface---a \textit{trait} in Rust---which would then be implemented by both
the \code{State} variants. This helps maintain the current status quo where
\code{State} is an \textit{abstract data type}: internal Soup callers do not
need to be aware of the location their data is getting stored---they can simply
interact with the \code{State} trait as a black box\@.

\begin{listing}[H]\label{lst:state}
  \begin{minted}[frame=lines]{rust}
pub trait State {
    /// Add an index keyed by the given columns.
    fn add_key(&mut self, columns: &[usize]);

    /// Inserts or removes each record into State
    fn process_records(&mut self, records: &Records);

    /// Retrieve values from the index defined for `columns`.
    fn lookup<'a>(
      &'a self,
      columns: &[usize],
      key: &KeyType
    ) -> LookupResult<'a>;

    /// Count the rows currently stored in `State`.
    fn rows(&self) -> usize;

    /// Return a copy of all records.
    fn cloned_records(&self) -> Vec<Vec<DataType>>;
}
  \end{minted}

  \caption{\
    A segment of the main methods defined in our \code{State} trait.
  }
\end{listing}

As code is often more succinct than prose, a subset of the \code{State} trait is
shown in listing~\ref{lst:state}. The rest of the methods have been omitted for
clarity, as they are only relevant to partially materialized state, which base
nodes never are (they are always fully materialized). Similarly, some of the
methods take in extra arguments related to partial state---omitted here.

Comparing the trait in listing~\ref{lst:state} to the operations described
in~\ref{sec:in-memory-state} you might notice that the insert and removal
operations are gone. These have been abstracted into a higher level method:
\code{process\_records}. Every packet in Soup has the potential to contain more
than one record by being a merged packet, due to \textit{group commit}
(see~\ref{sec:group-commit}). This meant that the function responsible for
materializing records in a node's \code{State} would go through a packet's
records, individually calling methods like \code{State::insert} and
\code{State::remove}.
This is completely okay for an in-memory implementation, but not as good for
something that might potentially write to slower, durable storage. Here batching
is key, and an extra indicator to the underlying methods that they can perform
operations in one go is crucial.

\subsection{Retrieving rows from \code{State}}
While other languages might implement memory safety through garbage collection
or manual memory management, Rust does the same through ownership (as described
in~\ref{sec:rust}). Whenever a value goes out of scope, it is deallocated. How
do you know when a value goes out of scope? In a garbage collected system, this
happens when there are no longer any references to the value. In Rust, each
value only has one owner, and any references need to live \textbf{at least} as
long as the value created by that owner.

However, what if a value needs to be owned by more than one location? That is
often the case for data structures, and \code{State} is no exception here.
Values stored in \code{MemoryState} should not have to be cloned during
retrieval, which would incur a heavy performance penalty. Instead, retrieving
values from \code{State} return dynamically reference
counted\footnote{\url{https://doc.rust-lang.org/std/rc/index.html}} values,
allowing shared ownership of a single value by counting owners at runtime.
Subsequent retrievals of the same value always point to the same memory
location, with the source of truth being stored in \code{State}.

What if, on the other hand, a row does not exist in memory to begin with? This
would be the case when data is retrieved from durable storage: after reading a
row in its on-disk representation and de-serializing it to a value in Soup's
data format, where is that very value stored? The \textit{data} used to create
the value exists on disk, but the actual memory representation of the value was
just created. While \code{MemoryState::lookup} would want to return a reference
to an internally stored value, \code{PersistentState::lookup} would rather want
to hand over ownership of the value to the caller. Enter the
\code{Cow}\footnote{\url{https://doc.rust-lang.org/std/borrow/enum.Cow.html}}
---a \textbf{clone-on-write} pointer from Rust's standard library that helps
with this exact purpose, by allowing data to be represented either as
\code{Borrowed} in the case of \code{MemoryState} or \code{Owned} for
\code{PersistentState}.

Additionally, the rows returned from \code{State::lookup} are wrapped in a
\code{LookupResult} enum, representing either a found or a missing value. The
latter is never relevant for \code{PersistentState}, which is only used to
represent fully materialized state. Note that a \textit{miss} does not signify
that the value does not exist, it simply means that this particular \code{State}
does not have it, while a \code{State} instance further up the data-flow graph
does.

\begin{listing}[H]\label{lst:existing-index}
  \begin{minted}[frame=lines]{rust}
// Before:
pub enum LookupResult<'a> {
    Some(&'a [Rc<Vec<DataType>>]),
    Missing,
}

// After:
pub enum RecordResult<'a> {
    Borrowed(&'a [Rc<Vec<DataType>>]),
    Owned(Vec<Vec<DataType>>),
}

pub enum LookupResult<'a> {
    Some(RecordResult<'a>),
    Missing,
}
  \end{minted}
  \caption{\
    Prior to the introduction of \code{PersistentState}, reads from \code{State}
    would always result in a borrowed, reference counted value. Now that only
    happens for reads from \code{MemoryState}---with \code{PersistentState} the
    caller is responsible for retaining ownership of the value.
  }
\end{listing}

\section{Persistent State with SQLite}

\section{Persistent State with RocksDB}
