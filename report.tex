\documentclass[b5paper,twoside]{report}

\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}

\usetikzlibrary{positioning}
\usetikzlibrary{shapes,decorations,shadows}

\begin{document}
\title{Efficient recovery in a data-flow based storage system}
\author{Martin Ek}
\date{\today}
\maketitle

\input{macros.tex}

\begin{abstract}
  Abstract
\end{abstract}

\tableofcontents
\pagebreak

\chapter{Introduction}

\chapter{Background}
\section{Soup}
Modern web applications face an increasingly difficult performance problem in
relation to their storage needs: traditional relational databases perform too
poorly when used in isolation. Either by failing to scale to a large amount of
concurrent users, or by taking too long to return results for more expensive
aggregation queries.

This is often worked around by building up intricate cache hierarchies
\cite{memcached}, similar to the multi-level cache hierarchy
readers might be accustomed to seeing in their processors. This solves both
performance problems: concurrent readers can
access materialized data from the upper levels of the cache hierarchy, achieving
low latency and high throughput even when faced with large amounts of clients.
Expensive queries then only need to be performed once, as long as the result is
invalidated when the underlying data changes. This touches upon the main problem
with these cache hierarchies: invalidation. Write operations still need to change
the underlying storage layer, but are now also responsible for clearing out or
updating the materialized cache levels.

This adds an extra layer of complexity, and is a trade-off accepted by almost
every modern day developer that faces more than a trivial amount of load to
their storage system.

Soup sets out to solve this by providing a single high performant system,
removing the need for manual materialization through cache hierarchies. The
current prototype scales to millions of writes and reads per second
\cite{soup} - both on a single machine and in a distributed fashion.

Soup does this by materializing data automatically in a data-flow
graph, building on existing streaming data-flow research \cite{naiad, dataflow}
by combining it with ideas from performant materialized view solutions
\cite{dbtoaster, pequod}.

\subsection{Data-flow Graph}
Soup turns base table schemas and a pre-defined list of queries into a data-flow
graph that doubles as a set of materialized views. The base tables form the
root nodes of the graph, and all writes propagate from here. The graph itself
resembles the query graph of a traditional relational database system
\cite{codd}, with the graph's intermediate nodes being relational operators.

However, whereas a relational database management system's query graph is used
primarily to retrieve data, by executing operators to fetch data from durable
storage, Soup's graph does the exact opposite. The graph is defined ahead of
queries being received, and writes stream through the relevant nodes in the
graph - starting from the base nodes. This skews the majority of the system's
work towards writes, as results are materialized at different nodes throughout
the graph.

The query graph includes at least one materialized node per pre-defined query,
located at the bottom of the graph as leaf nodes. This means that reading from
the system is a matter of finding the relevant leaf node, and retrieving data
from the node's materialized storage. This is one of the elements that make Soup
resemble more of a key-vaule store than a traditional RDBMS. Even though Soup
supports advanced SQL queries, both mutations and retrievals are done with a
key, which then maps to a corresponding value.

On an elementary level one could see Soup creating a separate graph for
each query, similar to query graphs in traditional database systems.

Soup could have made a separate graph for each query, but that would have been
quite inefficient. For one it would have lead to Soup materializing a lot of
duplicate data for intermediary materialized nodes, and base table writes would
have had to be propagated to more nodes than necessary. Migrations would also
have taken considerably longer time, as adding queries would mean constructing a
completely new query graph. Instead Soup uses multi-query optimization
techniques to create common sub-graphs of nodes that can be used in multiple
queries.

\subsection{Materialization}
Similar to in traditional relational databases, writes are inserted into the
table directly - a base node in Soup. However, whereas this marks the
end of the operation for an RDBMS, Soup will continue by propagating the write
throughout the query graph, storing the result at materialized nodes.

The insertions are handled with a group-commit protocol, where records are
batched up and finally inserted into a durable log before being propagated to
the base node.

Soup primarily materializes leaf nodes in the graph, to enable fast read
queries. However, in some cases intermediary operators might also choose to
materialize the records they receive before they forward them through to their
child nodes. This is especially the case for stateful operators, such as
extremum queries, as these need to incrementally update their result - instead
of having to go through and re-calculate the state on every new record.

Nodes can also be partially materialized, which lets each node choose which rows
to keep in their internal state. This resembles a regular caching system, where
keys can be evicted after a period of inactivity.

\subsection{Migrations}
Relational databases require developers to pre-define a set of table schemas for
their application. Changes to these throughout a project's lifecycle are often
inevitable, however the process of performing migrations in a live
system without downtime is usually far from trivial \cite{stripe}. This is one
of the core issues Soup sets out to solve, by providing a database system that
lets developers change both the base table schemas and the pre-defined queries
at any point.

Soup constructs a new graph for added queries. This is done by finding overlaps
with the existing query graph nodes, to avoid duplicate materializations and
to reduce the amount of work done by each migration. Soup marks nodes as
partially materialized when possible, which lets the system start processing
requests for the newly defined query right away, by only fetching each key from
ancestor nodes when necessary. For fully materialized nodes, Soup will replay
all relevant base table writes from the closest materialized node. This
delays the point where Soup can start processing requests, but has the advantage
of not delaying subsequent requests to retrieve materializated keys later on.

Modifications to base table schemas are done in-place, by having Soup's base
nodes internally include the full history of columns for that specific table.
% TODO: default values

\subsection{Transactions}
An increasing amount of real-world database system users have found relaxed
consistency guarantees to be sufficient in the last few years, as it greatly
simplifies the problem of replicating large amounts of data across data centers
with high performance \cite{existential}. There are nonetheless without doubt
a wide variety of usecases for transactions, where the eventual consistency that
Soup offers is insufficient. To cater to these types of applications, Soup
allows developers to opt-in for transaction support, by defining their base
nodes as transactional.

Soup's transaction support is carefully designed to avoid slowing down
non-transactional workloads. Using optimistic concurrency control, Soup returns
timestamped tokens for reads, which can later be used to verify the validity of
the data that was read. Writes can only be performed at the end of a
transaction, when clients request their transactions to be committed. The
timestamp part of the token helps Soup validate if concurrent clients
interferred with the data, which lets the system notify the client by aborting
the ongoing transaction.

\section{Rust}
Rust is an open-source systems programming language that guarantees memory
safety while maintaining a minimal runtime. Rust is sponsored by Mozilla, where
it is used to develop Servo - a completely new browser engine.

When choosing a language, developers are often forced to compromise between
higher level abstractions and performance. Large and latency sensitivy projects
like databases often opt for the latter, through low-level languages like C,
which avoids expensive runtime safety checks. Rust removes this dilemma
altogether by providing developers with both the fine-tuned control and
performance they are used to in low level languages, while offering abstractions
developers might be familiar with from interepretted languages.

One of Rust's key features is providing compile time safety both in terms of
types and memory. The latter is done through an ownership model which lets
developers program mostly without thinking about memory allocation and
deallocation, without the lowered performance of using something like a garbage
collector. Each variable in Rust is assigned one and only one owner, and the
variable is deallocated - or dropped - when that owner goes out of scope.

% TODO: example

\section{Recovery}
\subsection{Logging in Soup}

\chapter{Snapshotting}
\chapter{Results}
\chapter{Discussion}

\bibliographystyle{acm}
\bibliography{sources}

\end{document}
