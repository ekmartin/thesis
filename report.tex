\documentclass[b5paper,twoside]{report}

\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\usepackage{tikz}

\usetikzlibrary{positioning}
\usetikzlibrary{shapes,decorations,shadows}

\begin{document}
\title{Efficient recovery in a data-flow based storage system}
\author{Martin Ek}
\date{\today}
\maketitle

\input{macros.tex}

\begin{abstract}
  Abstract
\end{abstract}

\tableofcontents
\pagebreak

\chapter{Introduction}

\chapter{Background}
\section{Soup}
Modern web applications face an increasingly difficult performance problem in
relation to their storage needs: traditional relational databases perform too
poorly when used in isolation. Either by failing to scale to a large amount of
concurrent users, or by taking too long to return results for more expensive
aggregation queries.

This is often worked around by building up intricate cache hierarchies
\cite{facebook}, similar to the multi-level cache hierarchy
readers might be accustomed to seeing in their processors. This solves both
performance problems: concurrent readers can
access materialized data from the upper levels of the cache hierarchy, achieving
low latency and high throughput even when faced with large amounts of clients.
Expensive queries then only need to be performed once, as long as the result is
invalidated when the underlying data changes. This touches upon the main problem
with these cache hierarchies: invalidation. Write operations still need to change
the underlying storage layer, but are now also responsible for clearing out or
updating the materialized cache levels.

This adds an extra layer of complexity, and is a trade-off accepted by almost
every modern day developer that faces more than a trivial amount of load to
their storage system.

Soup sets out to solve this by providing a single high performant system,
removing the need for manual materialization through cache hierarchies. The
current prototype scales to millions of writes and reads per second
\cite{soup} - both on a single machine and in a distributed fashion.

Soup does this by materializing data automatically in a data-flow
graph, building on existing streaming data-flow research \cite{naiad, dataflow}
by combining it with ideas from performant materialized view solutions
\cite{dbtoaster, pequod}.

\subsection{Data-flow Graph}
Soup turns base table schemas and a pre-defined list of queries into a data-flow
graph that doubles as a set of materialized views. The base tables form the
root nodes of the graph, and all writes propagate from here. The graph itself
resembles the query graph of a traditional relational database system
\cite{codd}, with the graph's intermediate nodes being relational operators.

However, whereas a relational database management system's query graph is used
primarily to retrieve data, by executing operators to fetch data from durable
storage, Soup's graph does the exact opposite. The graph is defined ahead of
queries being received, and writes stream through the relevant nodes in the
graph - starting from the base nodes. This skews the majority of the system's
work towards writes, as results are materialized at different nodes throughout
the graph.

The query graph includes at least one materialized node per pre-defined query,
located at the bottom of the graph as leaf nodes. This means that reading from
the system is a matter of finding the relevant leaf node, and retrieving data
from the node's materialized storage. This is one of the elements that make Soup
resemble more of a key-vaule store than a traditional RDBMS. Even though Soup
supports advanced SQL queries, both mutations and retrievals are done with a
key, which then maps to a corresponding value.

On an elementary level one could see Soup creating a separate graph for
each query, similar to query graphs in traditional database systems.

Soup could have made a separate graph for each query, but that would have been
quite inefficient. For one it would have lead to Soup materializing a lot of
duplicate data for intermediary materialized nodes, and base table writes would
have had to be propagated to more nodes than necessary. Migrations would also
have taken considerably longer time, as adding queries would mean constructing a
completely new query graph. Instead Soup uses multi-query optimization
techniques to create common sub-graphs of nodes that can be used in multiple
queries.

% TODO: write about data-flow operators

\subsection{Materialization}

\subsection{Migrations}
Relational databases require developers to pre-define a set of table schemas for
their application. Changes to these throughout a project's lifecycle are often
inevitable, however the process of performing migrations in a live
system without downtime is usually far from trivial \cite{stripe}. This is one
of the core issues Soup sets out to solve, by providing a database system that
lets developers change both the base table schemas and the pre-defined queries
at any point.

Soup constructs a new graph for added queries. This is done by finding overlaps
with the existing query graph nodes, to avoid duplicate materializations and
to reduce the amount of work done by each migration. Soup marks nodes as
partially materialized when possible, which lets the system start processing
requests for the newly defined query right away, by only fetching each key from
ancestor nodes when necessary. For fully materialized nodes, Soup will replay
all relevant base table writes from the closest materialized node. This
delays the point where Soup can start processing requests, but has the advantage
of not delaying subsequent requests to retrieve materializated keys later on.

Modifications to base table schemas are done in-place, by having Soup's base
nodes internally include the full history of columns for that specific table.
% TODO: default values

\subsection{Transactions}

\section{Rust}

\section{Recovery}
\subsection{Logging in Soup}

\chapter{Snapshotting}
\chapter{Results}
\chapter{Discussion}

\bibliographystyle{acm}
\bibliography{sources}

\end{document}
