\chapter{Recovery}

\section{Logs}

\section{Persistent Base Nodes}

RocksDB recovers content that resided in its in-memory MemTables at the time of
a crash by replaying entries from its write-ahead log. This is a stark
improvement from recovery using Soup's regular write-ahead log, where every
entry from the beginning of time has to be replayed. Data that is already
persisted to durable SS-tables at the time of a crash require no extra work---
they can be read in the same manner after crashing as before.

% What happens to Soup's partially materialized nodes? Similar to a regular
% caching solution, they start out completely empty after recovering. Subsequent
% requests gradually restore the nodes to a state resembling the one they were in
% before crashing. Fully materialized nodes require a complete view of the state
% at all times and need to be sent a full copy of the state when recovering. Both
% this, and the fact that partially materialized nodes start out empty, lead to
% reduced initial performance---a target of improvement addressed in the next
% section.

\section{Snapshotting}\label{sec:snapshotting}

The introduction of \code{PersistentState} in the last chapter greatly improves
the recovery situation for base nodes: instead of having to replay all log
entries from the beginning of time to get back to the state they had prior to
crashing, only a small subset of entries have to be reapplied. Unfortunately,
faster recovery of base nodes does not improve the situation for materialized
nodes further down the graph. Partial nodes have to trigger a large amount of
replays to refill their state early on, while fully materialized nodes need a
complete copy of the state altogether before serving any reads at all.

\code{PersistentState} lets base nodes instantly recover to a recent point in
time, capping recovery to the time it takes to go through recent updates. A
similar solution for all materialized state would let nodes recover to a recent
checkpoint, followed by re-application of log entries to become fully
up-to-date. In short, we need to be able to consistently \textit{snapshot} the
materialized nodes at any given point.

The implementation in the rest of this section is based on an earlier version of
this thesis, submitted as a part of ``TDT4501 Computer Science, Specialization
Project''.

\todo{should this cite my project thesis?}

\subsection{Challenges}

Main memory systems like VoltDB leverage checkpointing by persisting the
transactional state of committed transactions, using log sequence numbers to be
able to track which updates have been reflected on disk~\cite{voltdb-recovery}.
In Soup, state is materialized at a variety of nodes throughout the query graph,
and updates have no timestamps or sequence numbers attached to them. Updates
propagate through the graph asynchronously, and a specific update is likely to
reach different points in the graph at separate times. Taking a global snapshot
of the entire graph simultaneously would mean capturing nodes at different
logical points, as an update might be in the process of propagating throughout
the graph at the time that the snapshot is initiated.

Soup's way of asynchronously propagating updates through its query-graph
resembles the communication done in a distributed system. Being able to observe
the global state in a distributed system---where access to a common clock is
rare---is an immensely useful property, crucial to resolving a certain category
of problems, such as deadlock detection.

Chandy and Lamport first introduced the problem of acquiring a distributed
snapshot in~\cite{chandy-lamport}, which has since been the source of
inspiration for a wide variety of work within the field. Chandy and Lamport
presented a solution aimed at distributed systems using first-in first-out
channels, with preserved message ordering, by solving two main issues: deciding
when to take a snapshot, and which messages should be part of said snapshot.

The key insight in~\cite{chandy-lamport} was to introduce a marker message, used
as a separator between messages that should be included in the snapshot, and
messages that should not. Processes that receive a snapshot marker should
immediately take a snapshot of all messages received prior to the marker, and
forward the resulting state to a process capable of assembling all its received
local snapshots to a global view of the system. The channels' FIFO property
ensures the exclusion of messages arriving after the marker. The resulting
algorithm requires $ O(e) $ messages to initiate a snapshot, where $ e $
is the amount of edges in the graph. The messages can be sent out in parallel,
resulting in a $ O(d) $ guarantee to complete the snapshot, where $ d $ is the
diameter of the graph.

Lai and Yang~\cite{lai-yang} later extended this scheme with support for
non-FIFO channels with a solution that also removed the need for explicit
control messages, piggy-backing the required snapshot information onto existing
packets.

\subsection{Algorithm}

Taking a snapshot of a running Soup instance requires persisting the content of
each materialized node in the current data-flow graph. This needs to happen at
the same logical point in time---ensuring that every in-flight update is either
propagated to \textit{all} nodes in the query graph---or none of them, leading
to a consistent state after recovering from a failure. At the same time, taking
a snapshot should not incur a too heavy performance cost on the running system,
and should definitely not stop the system from processing updates
completely---for any period of time. This lets us derive a few base rules for
our snapshotting algorithm:

\begin{enumerate}
  \item Snapshots need to include exactly the same updates across the graph.
  \item Snapshotting should not significantly degrade the system's throughput.
  \item Snapshots should complete in a reasonable amount of time.
\end{enumerate}

We can then use these rules to build a snapshotting algorithm in incremental
steps, starting from an example that fails to meet the defined criteria.
Figure~\ref{fig:bad-example} shows an update propagating through the Soup query
graph. What would be the outcome if both of the partially materialized
nodes---shown in a blue color---would snapshot their state at the exact moment
shown in the graph? Whereas the leftmost domain has had time to process update
\code{A}, the rightmost one has not. The two domains are at different
\textit{logical} points in time, and the snapshots would fail our first rule.

What if the system as a whole instead waited for the update to completely
propagate through the graph before initiating the snapshot? This would
successfully follow the first rule, but fail the second: no new updates could be
served until the snapshot has completed across the graph, halting
the system's throughput.

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{bad-example}
  \caption{\
    An update \texttt{A} propagates through the domains in the query graph in an
    asynchronous manner. Domains 2 and 3 contain at least one materialized node,
    and should be snapshotted.
  }\label{fig:bad-example}
\end{figure}

\subsubsection{Synchronous snapshotting}

Soup's query graph forwards updates over ordered FIFO channels, making it
possible to rely on Chandy-Lamport's marker technique to determine which updates
should be considered a part of a snapshot. Domains that receive the
marker initiate the snapshot process right away, without any further processing
of updates. This results in a global snapshot taken at the same \textit{logical}
point in time, even if the actual snapshots were instantiated at different
\textit{physical points}.
