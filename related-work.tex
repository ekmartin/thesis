\chapter{Related work}\label{chap:related-work}

\todo{todo!}
\newpage

\section{Indexing}

Index structures are used by databases to facilitate efficient retrieval
queries. While a majority of traditional database systems maintain indices
separate from the data itself (which could be stored in \eg a heap
file~\cite{microsoft-heap, psql-heap}), it has become increasingly common to
co-locate rows with the index---often referred to as a clustered index.
Systems such as InnoDB~\cite{innodb-source} and comdb2~\cite{comdb2} rely
heavily on B-trees for both indexing and row storage, achieving overall decent
read performance on a wide variety of storage mediums.

At the same time, a gradual increase in write-intensive applications have
resulted in a myriad of log-structured merge tree based storage systems---a
data-structure which usually requires less write amplification than
B-trees~\cite{lsm-vs-b}. While LSM-tree based systems (\eg Google
Bigtable~\cite{bigtable} and Apache HBase~\cite{hbase}) provide excellent
availability and scalability, their key-value based APIs are restrictive, and
lack features such as secondary indexing.

Key-value APIs are sufficient for many applications, while others require more
advanced features. Google Spanner~\cite{spanner} and its open-source competitors
remove the need to compromise between strong consistency and scalability, and
provide an SQL-based query interface to its users. While Spanner combines an
implementation based on Bigtable with Paxos to provide distributed consistency,
CockroachDB~\cite{cockroach} and TiDB~\cite{tidb}---both open-source---do the
same with the LSM-tree key-value store RocksDB~\cite{rocksdb} and the Raft
consensus algorithm~\cite{raft}.

Both CockroachDB and TiDB implement advanced features (\eg replication and
sharding) as layered abstractions, with RocksDB's ordered key-value API at the
core. With clever key schemes and heavy use of RocksDB's iteration properties,
CockroachDB and TiDB can support secondary indices on top of RocksDB---a
well-supported and heavily tested library with reliable performance guarantees.
Other projects, such as SLIK~\cite{slik}, HyperDex~\cite{hyperdex}, and
Replex~\cite{replex}, implement secondary indexing as first class citizens in
new distributed key-value stores built from the ground up.

% This section investigates how systems such as CockroachDB and TiDB implement
% secondary index schemes on top of a key-value API, while taking a look at
% existing research in the same field.

\subsection{Secondary indices with LSM-trees}

LSM-tree systems achieve high write throughput in part by buffering updates in
memory, amortizing the disk write penalty across a batch of writes.
AsterixDB~\cite{asterix-storage} recognizes the effectiveness of the LSM-tree
approach, and applies the same technique to in-place update index structures.
The process---which they refer to as LSM-ification---lets AsterixDB build
secondary indices using data structures that generally perform better while
doing so, \eg B-trees. While this is an interesting approach, on-disk data
structures are far from trivial to implement, and additional data structures
undoubtedly increase a system's overall complexity---even if the data structures
are built on the same components.

Another approach is to build index schemes on top of the existing APIs provided
by LSM-trees, notably the \code{Get}, \code{Put}, \code{Delete}, and \code{Seek}
operations provided by systems such as RocksDB.\@ In~\cite{lsm-comparison},
LSM-tree index structures are split into two categories: \textit{standalone}
and \textit{embedded}. The former maintains secondary indices in separate
key-spaces, while the latter embeds the necessary information without
necessitating additional stored rows.

\subsubsection{Standalone Indices}

In traditional relational database management systems, indices are usually
maintained as separate data structures (\eg B-trees) with pointers to primary
key values. Standalone indexing in~\cite{lsm-comparison} stores pointers as
well, either in the same key-space as regular updates or in separate tables
(often referred to as \textit{column families} in LSM-based systems). These
pointers can be maintained either \textit{eagerly} or \textit{lazily}, \eg
either synchronously on the main-path or asynchronously in the background. The
latter introduces complexity to the system, but offers potentially improved
insertion and update performance in return~\cite{deli, pnuts}.

How the secondary index pointers are structured varies from scheme to scheme.
The perhaps most obvious way of doing so is to maintain a serialized list of
primary keys for each secondary index key, which is then retrieved and updated
on each insertion. This is referred to as a \textit{posting list} in
\cite{lsm-comparison}, and \textit{table-based} secondary indexing in
\cite{hbase-secondary}. In the latter, where a secondary index scheme is
implemented on top of HBase~\cite{hbase}, a serialized \code{TreeSet} is
maintained for each secondary index value. The \code{TreeSet} is updated
synchronously on new insertions, which \cite{lsm-comparison} refers to as
\textit{eager indexing}. The alternative, \textit{lazy indexing}, would instead
issue only an insertion for new values, and take care of the concatenation
either in the background or during read operations (\textit{merge operator} in
RocksDB~\cite{rocksdb-merge}). While the lazy alternative offers far greater
insert performance by avoiding random reads, it still requires potentially
costly list serialization and deserialization.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l}
    \toprule
    \textbf{id} & \textbf{brand} & \textbf{color} \\ \midrule
    1 & volvo & silver \\ \midrule
    2 & volvo & blue \\ \midrule
    3 & audi & red \\ \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{l l}
    \toprule
    \textbf{key} & \textbf{value} \\ \midrule
    volvo & [1, 2] \\ \midrule
    audi & [3] \\ \bottomrule
  \end{tabular}

  \caption{\
    A separate list of primary keys is maintained for each secondary index key.
  }\label{table:secondary-list}
\end{table}

Another alternative is to rely on the ordered iteration properties available in
LSM-tree based systems such as LevelDB~\cite{leveldb-iteration} and
RocksDB~\cite{rocksdb-iteration}. By suffixing secondary keys with unique
primary keys (composite keys in \cite{lsm-comparison}), the pointers can be
retrieved by iterating through all keys that start with a given secondary index
prefix, removing the need to store anything in the value portion at all. While
this requires care to make sure that values with the same prefix are ordered
next to each other, it completely removes the need for random reads when
inserting new values. This is similar to how systems such as
Spanner~\cite{spanner-sql}, TiDB~\cite{tidb-internal}, and
CockroachDB~\cite{cockroach-design} implement secondary indexes.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l}
    \toprule
    \textbf{id} & \textbf{brand} & \textbf{color} \\ \midrule
    1 & volvo & silver \\ \midrule
    2 & volvo & blue \\ \midrule
    3 & audi & red \\ \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{l l}
    \toprule
    \textbf{key} & \textbf{value} \\ \midrule
    volvo-1 & \\ \midrule
    volvo-2 & \\ \midrule
    audi-3 & \\ \bottomrule
  \end{tabular}

  \caption{\
    Secondary index keys are suffixed with the primary key they point to, and
    can be retrieved by iterating through all secondary index rows with the
    correct prefix.
  }\label{table:secondary-order}
\end{table}

\subsubsection{Embedded indices}

Instead of storing separate index pointers for secondary indices,
\cite{lsm-comparison} presents an alternative where bloom filters are used to
determine whether an on-disk block contains rows with a given secondary index
attribute or not. Queries then iterate through all blocks, referring to the
in-memory bloom filter to determine whether it requires scanning for potential
rows. To retrieve values from the LSM-tree memory buffer, a separate B-tree is
maintained in-memory for each secondary attribute.

Embedded indexing reduces the write-amplification when inserting new rows---no
extra index data needs to be persisted to disk. In turn, it reduces read
performance, as retrievals now need to consider every block available, even if
only ends up reading a small subset.

\section{Recovery}

Database researchers observed early on that users needed a way of
performing a series of operations as a unit, where the result would either be
made available to concurrent users as one, or not at all~\cite{bernstein}---a
transaction. At the same time, failures are inevitable in any system, and
ensuring that the result of previously \textit{committed} transactions still
remained after crashing was crucial. Together, these requirements formed
a subset of the ACID~\cite{acid} principles (atomicity, consistency, isolation,
and durability).

ARIES---Algorithms for Recovery and Isolation Exploting
Semantics\cite{aries}---has in-large remained the gold standard in transaction
recovery algorithms for three decades. ARIES persists all changes---regardless
of commit status---to a durable write-ahead log. During recovery, ARIES first
applies all missing updates from the log, before it finally reverts changes
belonging to uncommitted transactions. The former, \code{REDO}, maintains
durability, while the latter, \code{UNDO}, upholds atomicity. By sequentially
persisting all changes to the log, ARIES systems are free to write dirty pages
to durable storage at any point, and does not need to do so prior to committing.
Referred to as correspondingly \textit{steal} and \textit{no-force}, this allows
for high throughput processing at the price of increasing complexity.

While the logging structure varies from implementation to implementation, the
principle of a write-ahead log remains the same. By appending changes to a
persistent log prior to writing the actual changes, we avoid the performance
penalties of random writes to durable storage, while still ensuring durability
in the face of a potential crash. To maintain atomicity for transactions, we
also log enough information to either safely revert their changes, or fully
persist them after recovering. With the introduction of fast non-volatile
memory, the age old wisdom of preferring sequential writes over random updates
might slowly go away~\cite{mars, wbl}. Regardless, to build systems that perform
well on hardware most users have access to---still in-large spinning and
solid-state drives---the arguments in-favor of write-ahead logging still remain.
