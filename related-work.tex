\chapter{Related work}\label{chap:related-work}

This section reviews ideas relevant to the main contributions throughout this
thesis, both from existing research and from various industry implementations.
The first section investigates concepts crucial to building secondary indexing
schemes on top of key-value stores, implemented in
chapter~\ref{chap:persistent-bases}. The second section looks at recovery
solutions for main-memory databases, followed by a dive into snapshotting
techniques in distributed systems, necessary for chapter~\ref{chap:recovery}.

\newpage

\section{Indexing}

Index structures are used by databases to facilitate efficient retrieval
queries. While a majority of traditional database systems maintain indices
separate from the data itself (which could be stored in \eg a heap
file~\cite{microsoft-heap, psql-heap}), it has become increasingly common to
co-locate rows with the index---often referred to as a clustered index.
Systems such as InnoDB~\cite{innodb-source} and comdb2~\cite{comdb2} rely
heavily on B-trees for both indexing and row storage, achieving overall decent
read performance on a wide variety of storage mediums.

At the same time, a gradual increase in write-intensive applications have
resulted in a myriad of log-structured merge tree based storage systems---a
data-structure which usually requires less write amplification than
B-trees~\cite{lsm-vs-b}. While LSM-tree based systems (\eg Google
Bigtable~\cite{bigtable} and Apache HBase~\cite{hbase}) provide excellent
availability and scalability, their key-value based APIs are restrictive, and
lack features such as secondary indexing.

Key-value APIs are sufficient for many applications, while others require more
advanced features. Google Spanner~\cite{spanner} and its open-source competitors
remove the need to compromise between strong consistency and scalability, and
provide an SQL-based query interface to its users. While Spanner combines an
implementation based on Bigtable with Paxos to provide distributed consistency,
CockroachDB~\cite{cockroach} and TiDB~\cite{tidb}---both open-source---do the
same with the LSM-tree key-value store RocksDB~\cite{rocksdb} and the Raft
consensus algorithm~\cite{raft}.

Both CockroachDB and TiDB implement advanced features (\eg replication and
sharding) as layered abstractions, with RocksDB's ordered key-value API at the
core. With clever key schemes and heavy use of RocksDB's iteration properties,
CockroachDB and TiDB can support secondary indices on top of RocksDB---a
well-supported and heavily tested library with reliable performance guarantees.
Other projects, such as SLIK~\cite{slik}, HyperDex~\cite{hyperdex}, and
Replex~\cite{replex}, implement secondary indexing as first class citizens in
new distributed key-value stores built from the ground up.

% This section investigates how systems such as CockroachDB and TiDB implement
% secondary index schemes on top of a key-value API, while taking a look at
% existing research in the same field.

\subsection{Secondary indices with LSM-trees}

LSM-tree systems achieve high write throughput in part by buffering updates in
memory, amortizing the disk write penalty across a batch of writes.
AsterixDB~\cite{asterix-storage} recognizes the effectiveness of the LSM-tree
approach, and applies the same technique to in-place update index structures.
The process---which they refer to as LSM-ification---lets AsterixDB build
secondary indices using data structures that generally perform better while
doing so, \eg B-trees. While this is an interesting approach, on-disk data
structures are far from trivial to implement, and additional data structures
undoubtedly increase a system's overall complexity---even if the data structures
are built on the same components.

Another approach is to build index schemes on top of the existing APIs provided
by LSM-trees, notably the \code{Get}, \code{Put}, \code{Delete}, and \code{Seek}
operations provided by systems such as RocksDB.\@ In~\cite{lsm-comparison},
LSM-tree index structures are split into two categories: \textit{standalone}
and \textit{embedded}. The former maintains secondary indices in separate
key-spaces, while the latter embeds the necessary information without
necessitating additional stored rows.

\subsubsection{Standalone Indices}

In traditional relational database management systems, indices are usually
maintained as separate data structures (\eg B-trees) with pointers to primary
key values. Standalone indexing in~\cite{lsm-comparison} stores pointers as
well, either in the same key-space as regular updates or in separate tables
(often referred to as \textit{column families} in LSM-based systems). These
pointers can be maintained either \textit{eagerly} or \textit{lazily}, \eg
either synchronously on the main-path or asynchronously in the background. The
latter introduces complexity to the system, but offers potentially improved
insertion and update performance in return~\cite{deli, pnuts}.

How the secondary index pointers are structured varies from scheme to scheme.
The perhaps most obvious way of doing so is to maintain a serialized list of
primary keys for each secondary index key, which is then retrieved and updated
on each insertion. This is referred to as a \textit{posting list} in
\cite{lsm-comparison}, and \textit{table-based} secondary indexing in
\cite{hbase-secondary}. In the latter, where a secondary index scheme is
implemented on top of HBase~\cite{hbase}, a serialized \code{TreeSet} is
maintained for each secondary index value. The \code{TreeSet} is updated
synchronously on new insertions, which \cite{lsm-comparison} refers to as
\textit{eager indexing}. The alternative, \textit{lazy indexing}, would instead
issue only an insertion for new values, and take care of the concatenation
either in the background or during read operations (\textit{merge operator} in
RocksDB~\cite{rocksdb-merge}). While the lazy alternative offers far greater
insert performance by avoiding random reads, it still requires potentially
costly list serialization and deserialization.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l}
    \toprule
    \textbf{id} & \textbf{brand} & \textbf{color} \\ \midrule
    1 & volvo & silver \\ \midrule
    2 & volvo & blue \\ \midrule
    3 & audi & red \\ \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{l l}
    \toprule
    \textbf{key} & \textbf{value} \\ \midrule
    volvo & [1, 2] \\ \midrule
    audi & [3] \\ \bottomrule
  \end{tabular}

  \caption{\
    A separate list of primary keys is maintained for each secondary index key.
  }\label{table:secondary-list}
\end{table}

Another alternative is to rely on the ordered iteration properties available in
LSM-tree based systems such as LevelDB~\cite{leveldb-iteration} and
RocksDB~\cite{rocksdb-iteration}. By suffixing secondary keys with unique
primary keys (composite keys in \cite{lsm-comparison}), the pointers can be
retrieved by iterating through all keys that start with a given secondary index
prefix, removing the need to store anything in the value portion at all. While
this requires care to make sure that values with the same prefix are ordered
next to each other, it completely removes the need for random reads when
inserting new values. This is similar to how systems such as
Spanner~\cite{spanner-sql}, TiDB~\cite{tidb-internal}, and
CockroachDB~\cite{cockroach-design} implement secondary indexes.

\begin{table}[H]
  \centering
  \begin{tabular}{l l l}
    \toprule
    \textbf{id} & \textbf{brand} & \textbf{color} \\ \midrule
    1 & volvo & silver \\ \midrule
    2 & volvo & blue \\ \midrule
    3 & audi & red \\ \bottomrule
  \end{tabular}
  \quad
  \begin{tabular}{l l}
    \toprule
    \textbf{key} & \textbf{value} \\ \midrule
    volvo-1 & \\ \midrule
    volvo-2 & \\ \midrule
    audi-3 & \\ \bottomrule
  \end{tabular}

  \caption{\
    Secondary index keys are suffixed with the primary key they point to, and
    can be retrieved by iterating through all secondary index rows with the
    correct prefix.
  }\label{table:secondary-order}
\end{table}

\subsubsection{Embedded indices}

Instead of storing separate index pointers for secondary indices,
\cite{lsm-comparison} presents an alternative where bloom filters are used to
determine whether an on-disk block contains rows with a given secondary index
attribute or not. Queries then iterate through all blocks, referring to the
in-memory bloom filter to determine whether it requires scanning for potential
rows. To retrieve values from the LSM-tree memory buffer, a separate B-tree is
maintained in-memory for each secondary attribute.

Embedded indexing reduces the write-amplification when inserting new rows---no
extra index data needs to be persisted to disk. In turn, it reduces read
performance, as retrievals now need to consider every block available, even if
only ends up reading a small subset.

\section{Recovery}

Database researchers observed early on that users needed a way of
performing a series of operations as a unit, where the result would either be
made available to concurrent users as one, or not at all~\cite{bernstein}---a
transaction. At the same time, failures are inevitable in any system, and
ensuring that the result of previously \textit{committed} transactions still
remained after crashing was crucial. Together, these requirements formed
a subset of the ACID~\cite{acid} principles (atomicity, consistency, isolation,
and durability).

ARIES---Algorithms for Recovery and Isolation Exploiting
Semantics~\cite{aries}---has in-large remained the gold standard in transaction
recovery algorithms for three decades. ARIES persists all changes---regardless
of commit status---to a durable write-ahead log. During recovery, ARIES first
applies all missing updates from the log, before it finally reverts changes
belonging to uncommitted transactions. The former, \code{REDO}, maintains
durability, while the latter, \code{UNDO}, upholds atomicity. By sequentially
persisting all changes to the log, ARIES systems are free to write dirty pages
to durable storage at any point, and does not need to do so prior to committing.
Referred to as correspondingly \textit{steal} and \textit{no-force}, this allows
for high throughput processing at the price of increasing complexity.

While the logging structure varies from implementation to implementation, the
principle of a write-ahead log remains the same. By appending changes to a
persistent log prior to writing the actual changes, we avoid the performance
penalties of random writes to durable storage, while still ensuring durability
in the face of a potential crash. To maintain atomicity for transactions, we
also log enough information to either safely revert their changes, or fully
persist them after recovering. With the introduction of fast non-volatile
memory, the age old wisdom of preferring sequential writes over random updates
might slowly go away~\cite{mars, wbl}. Regardless, to build systems that perform
well on hardware most users have access to---still in-large spinning and
solid-state drives---the arguments in-favor of write-ahead logging still remain.

\subsection{Recovery in main-memory databases}

Traditional relational database systems were never built with the goal of
storing entire datasets in main-memory. B-trees, concurrency control techniques,
buffer pools, and other components were instead built with the opposite in
mind---efficient processing of data residing on slow durable storage mediums.
Today's cheap access to vast amounts of volatile main-memory requires different
thinking, which has given rise to a new type of structured storage system:
main-memory databases~\cite{main-memory}.

According to \cite{oltp}, the SHORE\furl{http://research.cs.wisc.edu/shore-mt/}
database system spends over 10\% of its processing time maintaining an
ARIES-style log. For main-memory systems capable of processing thousands of
transactions per second, the penalty of writing to durable storage would be far
beyond 10\%. Regardless, main-memory database systems still need to ensure
durability somehow, otherwise they would merely be large data structures. VoltDB
introduced the concept of \textit{command logging}~\cite{voltdb-recovery}, where
the operation performed is logged instead of the results of its modifications.

Logging logical operations, \eg SQL queries, reduces the amount of data written
to durable storage. Whereas an ARIES-style log would have to write the results
of the operations performed, a command log would only need to persist the intent
of the operation itself. Maintaining a command log reduces the computational
overhead of durability, by removing the need to calculate before and after
images of modifications. VoltDB, HyPer~\cite{hyper}, and Hekaton~\cite{hekaton}
group operations from multiple transactions together in a single batch before
writing to durable storage, amortizing the fixed cost of syncing to persistent
storage across multiple transactions. Since main-memory systems never write
dirty pages to disk, they do not have to make sure that log entries are written
prior to committing, as a failure would not require undoing changes on durable
storage. Instead, they have to delay write acknowledgments until the entire
batch has been persisted. Batching updates with a \textit{group commit} scheme
greatly increases a system's write throughput, at the expense of potentially
increased latency of individual operations.

Command logging does on the other hand increase the effort needed to recover
from a failure. Whereas an ARIES-style log contains the computed results of each
operation, a command log does not, and needs to redo potentially costly
computations while recovering. VoltDB, however, claims that failures are rare,
and that the focus should be on reducing run-time overhead, even if it comes at
the cost of increased recovery latency. Regardless, recovering from an
ever-growing log of entries is not a feasible choice. Applications with
consistently high throughput would never be able to recover, as they would have
to redo all operations that happened since the beginning of time.

To avoid an infinitely growing write-ahead log, main-memory databases
\textit{checkpoint} their state at regular intervals~\cite{main-memory, dali,
margaret, voltdb-recovery, hekaton}. While far from a new concept, increasing
performance demands have led to the development of more sophisticated checkpoint
methods~\cite{memory-checkpoint, siren}. The implementation details vary, but
most systems agree that checkpoints should be performed in an asynchronous
manner, without blocking the system, and without inducing a significant
performance penalty. From this, \cite{memory-checkpoint} defines a few key
properties:

\begin{enumerate}
  \item Checkpointing should not significantly slow down transactional throughput.
  \item Checkpointing should not drastically increase regular processing latency.
  \item Checkpointing should require as little extra memory as possible.
\end{enumerate}

While the overhead of an algorithm following the aforementioned properties is
minimal, the asynchronous checkpoint method presented in
\cite{memory-checkpoint} still introduces a window of time where throughput
degrades with 10\%. To completely avoid the overhead of durable storage,
main-memory databases like H-Store rely on replication for
durability~\cite{hstore}. By replicating data to $ K + 1 $ nodes, the system
maintains transactional durability for up to $ K $ failures.
\textit{K-safety}~\cite{cstore} is however naturally susceptible to total
failures, \eg in the event that multiple data centers fail. As a compromise,
some systems implement replication together with checkpointing, limiting the
amount of data lost in a total failure.

\subsection{Snapshotting in distributed systems}\label{sec:rel-snapshotting}

Determining the global state of a distributed system is a useful property in
scenarios ranging from steady-state detection of deadlocks, monitoring,
debugging, and finally, failure recovery~\cite{intro-snapshot}---the aspect of
which snapshotting is used for in this thesis. By combining local checkpoints
across the Soup data-flow graph, a global checkpoint---a snapshot---can be
formed, and later recovered from in case of failures.

To perform a global snapshot, each node needs to record their local state at the
same instant across the system, without sharing memory or access to a global
clock. At the same time, the snapshot should happen without pausing regular
processing. Chandy and Lamport first introduced the problem of acquiring a
distributed snapshot in~\cite{chandy-lamport}, which has since been the source
of inspiration for a wide variety of work within the field. Chandy and Lamport
presented a solution aimed at distributed systems using first-in first-out
channels, with preserved message ordering, by solving two main issues: deciding
when to take a snapshot, and which messages should be part of said snapshot.

\begin{figure}
\begin{displayquote}
  ``\textit{The state-detection algorithm plays the role of a group of photographers
observing a panoramic, dynamic scene, such as a sky filled with migrating birdsa
scene so vast that it cannot be captured by a single photograph. The photographers
must take several snapshots and piece the snapshots together to form a
picture of the overall scene. The snapshots cannot all be taken at precisely the
same instant because of synchronization problems. Furthermore, the photographers
should not disturb the process that is being photographed; for instance,
they cannot get all the birds in the heavens to remain motionless while the
photographs are taken. Yet, the composite picture should be meaningful. The
problem before us is to define “meaningful” and then to determine how the
photographs should be taken.}''
\end{displayquote}
  \caption{Chandy and Lamport's description of the global snapshotting problem.}
\end{figure}

Chandy-Lamport's key insight was to introduce a marker message, used as a
separator between messages that should be included in the snapshot, and messages
that should not. Processes that receive a snapshot marker should immediately
take a snapshot of all messages received prior to the marker, and forward the
resulting state to a process capable of assembling all its received local
snapshots to a global view of the system. The channels' FIFO property ensures
the exclusion of messages arriving after the marker. The resulting algorithm
requires $ O(e) $ messages to initiate a snapshot, where $ e $ is the amount of
edges in the graph. The messages can be sent out in parallel, resulting in a $
O(d) $ guarantee to complete the snapshot, where $ d $ is the diameter of the
graph.

Later on, Spezialetti and Kearns improved the Chandy-Lamport algorithm by
recognizing that the combining phase of snapshotting could be performed
concurrently across the graph~\cite{spez-k}. Instead of having all nodes send a
snapshot to a single initiator, each node in the graph would be assigned a
\textit{parent} which it would forward its local state to, forming a spanning
tree of initiators across the graph.

Chandy and Lamport's algorithm is designed with FIFO channels in mind, and a
second class of snapshotting algorithms extend Chandy-Lamport in various ways
for non-FIFO channels. Lai and Yang~\cite{lai-yang} introduced a solution that
removes the need for explicit control messages, by including the required
snapshotting information in existing messages. To maintain consistency without
explicit marker messages, Lai-Yang makes use of a two-coloring scheme. Every
process is initially white and turns red when initiating a snapshot, while
recording all messages since the last snapshot was taken. Another alternative is
the Mattern algorithm~\cite{mattern}, which makes use of vector clocks to
perform global snapshots in non-FIFO environments.

Systems that uphold causal ordering of all messages open for a third category of
snapshotting algorithms. A causally ordered system guarantees that for two
messages $ m_1 $ and $ m_2 $, if $ sent(m_1) < send(m_2) $ then $ deliver(m_1) <
deliver(m_2) $ for any common destinations of both $ m_1 $ and $ m_2 $. This
removes the need for Chandy-Lamport's explicit synchronization markers, and both
Acharya-Badrinath~\cite{acharya} and Alagar-Venkatesan~\cite{alagar} present
snapshotting algorithms where the initiator requests a snapshot directly from
each node, reducing the required messages from $ O(e) $ to $ O(n) $.

% TODO: write about Consistency Issues in Distributed Checkpoints
