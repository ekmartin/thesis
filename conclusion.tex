\chapter{Conclusion}\label{chap:conclusion}

\section{Conclusion}

\section{Future work}\label{sec:future-work}

\subsection{Snapshotting and persistent bases}

The \code{PersistentState} implementation described
in~\ref{chap:persistent-bases} removes the regular Soup write-ahead log in favor
of relying on RocksDB for durability. RocksDB maintains its own WAL,
which---unlike Soup's---is discarded when its updates are safely flushed to
durable storage. This is far better for recovery purposes, as it avoids the need
to go through a seemingly endless stream of updates to restore Soup back to a
pre-failure state. It does, on the other hand, complicate matters for
snapshotting.

Snapshotting relies on Soup's write-ahead log to recover updates that occur
after a snapshot is taken, prior to a failure. During recovery, the latest
snapshot is first restored, followed by log-based recovery for any remaining log
entries. Together they make sure that Soup recovers quickly, without degrading
its durability guarantees. With persistent bases the write-ahead log is
maintained internally by RocksDB, together with the decision of when to
eventually discard prior log files. Without the ability to replay log entries,
recovering using snapshotting would leave all other nodes than the base nodes in
an older state than before the crash.

Recovery using persistent bases leaves the partial nodes further down the graph
empty. This works fine because of Soup's replay system: any missing reads will
propagate all the way to the base nodes, resulting in the partial nodes
eventually reaching a similar state to the one they were in prior to crashing.
With snapshotting, the partial nodes would end up in an \textit{old} state,
instead of empty. This would prevent Soup from issuing base node replays,
effectively discarding the updates that happened after the last snapshot was
taken.

That leaves the question of how to replay any updates that happened after the
last snapshot was taken, while still relying on RocksDB's write-ahead log for
persistence. The first step would be to ensure that RocksDB never discards WAL
files until all its updates are included in a snapshot. Secondly, Soup's
recovery procedure would need to retrieve updates that happened after the last
snapshot was taken, directly from the RocksDB write-ahead logs. By including the
current snapshot identifier in all persisted updates, the recovery process would
be able to discern between updates that happened before and after the last
persisted snapshot.

\subsection{\code{PersistentState} serialization}

Both the keys and values persisted to RocksDB are serialized using
bincode (see section~\ref{sec:bincode}). While bincode performs well
compared to other serialization libraries,

\subsection{Uncoordinated snapshots}

Coordinating a global snapshot across the entire data-flow graph requires
unnecessary communication between the workers and the controller. Instead, the
question of finding the last valid snapshot could be left to the recovery
process, \eg by finding $ Min(epoch) $ across the nodes, or by following schemes
such as~\cite{falkirk}.

\subsection{Incremental snapshots}

The write-performance benchmark in section~\ref{sec:snapshot-write} showed a
10\% decrease in overall write throughput after introducing snapshotting. The
majority of the work is performed in separate snapshotting threads, leaving the
state clone operation as the culprit. To avoid cloning altogether, snapshots
would need to be maintained gradually, which could be achieved by maintaining a
buffer of changes between snapshots, which could then be forwarded to the
snapshotting worker and applied there. While this would avoid the need to clone
the entire state, snapshot workers would now need to keep an entirely duplicate
clone of the snapshot state in memory, effectively doubling Soup's memory usage.

Instead, snapshots could be maintained incrementally directly on durable
storage. This could make use of the same \code{PersistentState} implementation
used by persistent base nodes, either by having the snapshotting workers apply
received updates to RocksDB, or by doing so directly from each domain. This
would significantly reduce the write-amplification required to persist a
snapshot, by avoiding the need to write duplicate data to disk again and again.
By doing so, it would also minimize the risk of filling up the snapshot workers'
queues, which could now happen if the time it takes to serialize and persist a
single, possibly large, snapshot grows beyond the predefined snapshot interval.
