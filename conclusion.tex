\chapter{Conclusion}\label{chap:conclusion}

This chapter looks back at the results presented earlier in this thesis, drawing
conclusions from both the positive and the negative side-effects of the
contributed implementations. Afterwards, it takes a look at possible durability
directions Soup might take in the future, towards the eventual goal of becoming
a production-ready system.

\newpage

\section{Persistent base tables}

Storing base tables on durable storage, instead of in volatile memory, lets Soup
handle continuously increasing quantities of data without having its memory
usage grow without bounds. With base tables stored safely on durable storage,
Soup no longer needs to replay all previous updates to recover after a failure.
Read requests instead replay data from the base tables when needed, eventually
bringing Soup's partially materialized views back to a similar state they were
in prior to the failure.

The persistent base table implementation contributed throughout this thesis
replaces Soup's in-memory index structure with a durable index structure built
on top of RocksDB---a battle-tested key-value storage layer developed at
Facebook. Compared to Soup's previous ever-growing write-ahead log, the
RocksDB-based implementation decreases the raw write throughput with about 10\%,
while moving Soup from a pure main-memory database to a storage layer capable of
handling data larger than its resident memory size, with significant benefits
for both recovery and Soup's overall memory usage. The implementation is
currently used by the latest Soup prototype.

\section{Snapshotting}

With base tables persisted to durable storage, Soup can begin serving requests
shortly after recovering from a failure. Unfortunately, these requests will need
to go through the entire data-flow graph before returning, with Soup's partially
materialized views starting out empty after recovering. Akin to the performance
penalty induced by a cold cache, Soup performs read requests with higher latency
until the application's working set again is present in the partially
materialized nodes throughout the graph. Snapshotting avoids this by restoring
all materialized state, after which Soup can continue serving read operations as
if nothing happened.

Compared to log-based recovery, the snapshotting implementation described
throughout this thesis recovers in less than a tenth of the time, at the cost of
a 10\% overall penalty to Soup's write throughput. While rapid recovery is an
important property for a database system, the performance penalty induced here
comes with considerably less benefits than moving Soup's base tables to durable
storage, leaving the question of whether it is a reasonable compromise open. At
the same time, the effects of snapshotting over a longer period of time have yet
to be investigated and should be considered---especially in regards to the
proposed improvements to the current snapshotting method. All in all,
snapshotting is a promising concept, with the possibility of being an important
part of a production-ready Soup system in the future.

\section{Conclusion}

This thesis presents the internals of the Soup structured storage system and the
challenges faced by its current solution for maintaining write durability. Prior
to the contributions described throughout this thesis, Soup was a pure
main-memory database, incapable of handling datasets beyond the size of its
available memory. Now, Soup stores the majority of its data in durable index
structures, while maintaining much of the same performance guarantees as before.
Finally, Soup recovers considerably faster after failures, as it no longer has
to go through and re-apply all updates from its previously ever-growing
write-ahead log.

\section{Future work}\label{sec:future-work}

This section presents possibly paths of improvement for the two main
contributions presented throughout this thesis.

\subsection{Snapshotting and persistent bases}

The \code{PersistentState} implementation described in
section~\ref{chap:persistent-bases} removes the regular Soup write-ahead log in
favor of relying on RocksDB for durability. RocksDB maintains its own WAL,
which, unlike the Soup WAL, is discarded when its updates are safely flushed to
durable storage. This is far better for recovery purposes, as it avoids the need
to go through a seemingly endless stream of updates to restore Soup back to a
pre-failure state. It does, on the other hand, complicate matters for
snapshotting.

Snapshotting relies on Soup's write-ahead log to recover updates that occur
after a snapshot is taken, prior to a failure. During recovery, the latest
snapshot is first restored, followed by log-based recovery for any remaining log
entries. Together they make sure that Soup recovers quickly, without degrading
its durability guarantees. With persistent bases the write-ahead log is
maintained internally by RocksDB, together with the decision of when to
eventually discard prior log files. Without the ability to replay log entries,
recovering using snapshotting would leave all other nodes than the base nodes in
an older state than before the crash.

Recovery using persistent bases leaves the partial nodes further down the graph
empty. This works fine because of Soup's replay system: any missing reads will
propagate all the way to the base nodes, resulting in the partial nodes
eventually reaching a similar state to the one they were in prior to crashing.
With snapshotting, the partial nodes would end up in an \textit{old} state,
instead of empty. This would prevent Soup from issuing base node replays,
effectively discarding the updates that happened after the last snapshot was
taken.

That leaves the question of how to replay updates that happened after the last
snapshot was taken, while still relying on RocksDB's write-ahead log for
persistence. The first step would be to ensure that RocksDB never discards WAL
files until all its updates are included in a snapshot. Secondly, Soup's
recovery procedure would need to retrieve updates that happened after the last
snapshot was taken, directly from the RocksDB write-ahead logs. By including the
current snapshot identifier in all persisted updates, the recovery process would
be able to discern between updates that happened before and after the last
persisted snapshot.

\subsection{\code{PersistentState} serialization}

Both the keys and values persisted to RocksDB are serialized using bincode (see
section~\ref{sec:bincode}). While bincode performs well compared to other
serialization libraries, implementing encoding techniques specifically for
Soup's use case would come with other potential benefits. One example is
specific sorting orders in \code{PersistentState}. With the current
implementation, keys would have to be deserialized before they could be compared
to each other, resulting in unnecessary allocations. These allocations would be
avoided by using an encoding scheme where keys could be compared without
deserialization, such as \eg MyRock's \code{memcomparable}
format~\cite{myrocks-encoding}.

\subsection{Uncoordinated snapshots}

Coordinating a global snapshot across the entire data-flow graph requires
unnecessary communication between the workers and the controller. Instead, the
question of finding the last valid snapshot could be left to the recovery
process, \eg by finding $ Min(epoch) $ across the nodes, or by following schemes
such as~\cite{falkirk}.

\subsection{Incremental snapshots}

The write-performance benchmark in section~\ref{sec:snapshot-write} showed a
10\% decrease in overall write throughput after introducing snapshotting. The
majority of the work is performed in separate snapshotting threads, leaving the
state clone operation as the culprit. To avoid cloning altogether, snapshots
would need to be maintained gradually, which could be achieved by maintaining a
buffer of changes between snapshots, which could then be forwarded to the
snapshotting worker and applied there. While this would avoid the need to clone
the entire state, snapshot workers would now need to keep an entirely duplicate
clone of the snapshot state in memory, effectively doubling Soup's memory usage.

Instead, snapshots could be maintained incrementally directly on durable
storage. This could make use of the same \code{PersistentState} implementation
used by persistent base nodes, either by having the snapshotting workers apply
received updates to RocksDB, or by doing so directly from each domain. This
would significantly reduce the write-amplification required to persist a
snapshot, by avoiding the need to write duplicate data to disk again and again.
Incremental snapshotting would also minimize the risk of filling up the snapshot
workers' queues, which could now happen if the time it takes to serialize and
persist a single, possibly large, snapshot grows beyond the predefined snapshot
interval.
