\chapter{Conclusion}\label{chap:conclusion}

This chapter looks back at the results presented earlier in this thesis, drawing
conclusions from both the positive and the negative side-effects of the
contributed implementations. Afterwards, it takes a look at possible durability
directions Soup might take in the future, towards the eventual goal of becoming
a production-ready system.

\newpage

\section{Persistent bases}

Storing Soup's base tables on durable storage instead of in-memory lets Soup
handle continuously increasing quantities of data without having its
memory-usage grow without bounds. With base tables safely stored on durable
storage, Soup no longer needs to replay all previous updates to recover after a
failure. Read-operations instead replay data from the base tables when needed,
eventually bringing Soup's partially materialized views back to a similar state
they were in prior to the failure.

The persistent base table implementation contributed throughout this thesis
replaces Soup's in-memory index structure with a durable index structure built
on top of RocksDB---a battle-tested key-value storage layer developed at
Facebook. Compared to Soup's previous ever-growing write-ahead log, the
RocksDB-based implementation decreases the raw write-throughput with about 10\%,
while providing significant benefits both for recovery and memory-usage. The
implementation is currently used by the latest Soup prototype.

\section{Snapshotting}

With base tables persisted to durable storage, Soup can begin serving requests
shortly after recovering from a failure. Unfortunately, these requests will need
to go through the entire data-flow graph before returning, with Soup's partially
materialized views starting out empty after recovering. Akin to the performance
penalty a cold cache induces, Soup will perform read requests with higher
latency until the application's working set again is present in the partially
materialized nodes throughout the graph. Snapshotting avoids this by restoring
all materialized state, after which Soup can continue serving read operations as
if nothing happened.

Compared to log-based recovery, the snapshotting implementation described in
section~\ref{sec:snapshotting} recovers in less than a tenth of the time, while
inducing a small 10\% overall penalty to Soup's write-throughput. At the same
time, there are cases that potentially produce consistency issues in the current
snapshotting algorithm, which would need to be resolved.

\todo{this needs a bit more info, and a more concluding conclusion}

\todo{this should probably have a section that concludes what the future of
durable soup is (e.g., replication?)}

\section{Future work}\label{sec:future-work}

\subsection{Snapshotting and persistent bases}

The \code{PersistentState} implementation described in
section~\ref{chap:persistent-bases} removes the regular Soup write-ahead log in
favor of relying on RocksDB for durability. RocksDB maintains its own WAL,
which, unlike the Soup WAL, is discarded when its updates are safely flushed to
durable storage. This is far better for recovery purposes, as it avoids the need
to go through a seemingly endless stream of updates to restore Soup back to a
pre-failure state. It does, on the other hand, complicate matters for
snapshotting.

Snapshotting relies on Soup's write-ahead log to recover updates that occur
after a snapshot is taken, prior to a failure. During recovery, the latest
snapshot is first restored, followed by log-based recovery for any remaining log
entries. Together they make sure that Soup recovers quickly, without degrading
its durability guarantees. With persistent bases the write-ahead log is
maintained internally by RocksDB, together with the decision of when to
eventually discard prior log files. Without the ability to replay log entries,
recovering using snapshotting would leave all other nodes than the base nodes in
an older state than before the crash.

Recovery using persistent bases leaves the partial nodes further down the graph
empty. This works fine because of Soup's replay system: any missing reads will
propagate all the way to the base nodes, resulting in the partial nodes
eventually reaching a similar state to the one they were in prior to crashing.
With snapshotting, the partial nodes would end up in an \textit{old} state,
instead of empty. This would prevent Soup from issuing base node replays,
effectively discarding the updates that happened after the last snapshot was
taken.

That leaves the question of how to replay updates that happened after the last
snapshot was taken, while still relying on RocksDB's write-ahead log for
persistence. The first step would be to ensure that RocksDB never discards WAL
files until all its updates are included in a snapshot. Secondly, Soup's
recovery procedure would need to retrieve updates that happened after the last
snapshot was taken, directly from the RocksDB write-ahead logs. By including the
current snapshot identifier in all persisted updates, the recovery process would
be able to discern between updates that happened before and after the last
persisted snapshot.

\subsection{\code{PersistentState} serialization}

Both the keys and values persisted to RocksDB are serialized using bincode (see
section~\ref{sec:bincode}). While bincode performs well compared to other
serialization libraries, implementing encoding techniques specifically for
Soup's use case would come with other potential benefits. One example is
specific sorting orders in \code{PersistentState}. With the current
implementation, keys would have to be deserialized before they could be compared
to each other, resulting in unnecessary allocations. These allocations would be
avoided by using an encoding scheme where keys could be compared without
deserialization, such as \eg MyRock's \code{memcomparable}
format~\cite{myrocks-encoding}.

\subsection{Uncoordinated snapshots}

Coordinating a global snapshot across the entire data-flow graph requires
unnecessary communication between the workers and the controller. Instead, the
question of finding the last valid snapshot could be left to the recovery
process, \eg by finding $ Min(epoch) $ across the nodes, or by following schemes
such as~\cite{falkirk}.

\subsection{Incremental snapshots}

The write-performance benchmark in section~\ref{sec:snapshot-write} showed a
10\% decrease in overall write throughput after introducing snapshotting. The
majority of the work is performed in separate snapshotting threads, leaving the
state clone operation as the culprit. To avoid cloning altogether, snapshots
would need to be maintained gradually, which could be achieved by maintaining a
buffer of changes between snapshots, which could then be forwarded to the
snapshotting worker and applied there. While this would avoid the need to clone
the entire state, snapshot workers would now need to keep an entirely duplicate
clone of the snapshot state in memory, effectively doubling Soup's memory usage.

Instead, snapshots could be maintained incrementally directly on durable
storage. This could make use of the same \code{PersistentState} implementation
used by persistent base nodes, either by having the snapshotting workers apply
received updates to RocksDB, or by doing so directly from each domain. This
would significantly reduce the write-amplification required to persist a
snapshot, by avoiding the need to write duplicate data to disk again and again.
Incremental snapshotting would also minimize the risk of filling up the snapshot
workers' queues, which could now happen if the time it takes to serialize and
persist a single, possibly large, snapshot grows beyond the predefined snapshot
interval.
